{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6wjDngDA4wM"
      },
      "outputs": [],
      "source": [
        "#wget \"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_fjfEH9CBys",
        "outputId": "a524cd66-1948-461b-ad95-00c2284f5d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're running Colab\n",
            "Colab: mounting Google drive on  /content/gdrive\n",
            "Mounted at /content/gdrive\n",
            "\n",
            "Colab: making sure  /content/gdrive/My Drive  exists.\n",
            "\n",
            "Colab: Changing directory to  /content/gdrive/My Drive\n",
            "/content/gdrive/My Drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False\n",
        "\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "if IN_COLAB:\n",
        "  # Mount the Google Drive at mount\n",
        "  mount='/content/gdrive'\n",
        "  print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "  drive.mount(mount)\n",
        "\n",
        "  # Switch to the directory on the Google Drive that you want to use\n",
        "  import os\n",
        "  drive_root = mount + \"/My Drive\"\n",
        "  #drive_root = mount + \"/My Drive/maskrcnn-benchmark\"\n",
        "  # Create drive_root if it doesn't exist\n",
        "  create_drive_root = True\n",
        "  if create_drive_root:\n",
        "    print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "    os.makedirs(drive_root, exist_ok=True)\n",
        "  \n",
        "  # Change to the directory\n",
        "  print(\"\\nColab: Changing directory to \", drive_root)\n",
        "  %cd $drive_root"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShehJ1JFBGD_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVGItI1MBICg"
      },
      "outputs": [],
      "source": [
        "#!unzip  fgvc-aircraft-2013b.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kMmN2IjCXIT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s4J-VsRRHPs"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXxxWZJhB9tg"
      },
      "outputs": [],
      "source": [
        "# Path\n",
        "data_path = 'fgvc-aircraft-2013b/data'\n",
        "img_path = 'fgvc-aircraft-2013b/data/images'\n",
        "\n",
        "# IMG_SIZE is determined by EfficientNet model choice\n",
        "IMG_SIZE = 240\n",
        "NUM_CLASSES = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sn06QRnRMd3"
      },
      "source": [
        "Image ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b-L-LSRKHyS",
        "outputId": "a84b5d52-750a-4116-9ebe-afa1e446a2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train, valid, test sizes: 3334 - 3333 - 3333\n"
          ]
        }
      ],
      "source": [
        "def read_id(filename):\n",
        "  img = []\n",
        "  f = open(filename, 'r', encoding='utf-8')\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    img.append(line.replace('\\n', ''))\n",
        "  f.close()\n",
        "  return img\n",
        "\n",
        "train_id = read_id(os.path.join(data_path, 'images_train.txt'))\n",
        "val_id = read_id(os.path.join(data_path, 'images_val.txt'))\n",
        "test_id = read_id(os.path.join(data_path, 'images_test.txt'))\n",
        "\n",
        "print('Train, valid, test sizes: {} - {} - {}'.format(len(train_id), len(val_id), len(test_id)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyaMs_5BqTu7"
      },
      "source": [
        "1. Read images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu-BLyBsrKUP"
      },
      "outputs": [],
      "source": [
        "bbox = {}\n",
        "f = open(os.path.join(data_path, 'images_box.txt'), 'r', encoding='utf-8')\n",
        "lines = f.readlines()\n",
        "for line in lines:\n",
        "  arr = line.replace('\\n', '').split(' ')\n",
        "  bbox[arr[0]] = [int(arr[i]) for i in range(1, 5)]\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP7uoewRqPrL"
      },
      "outputs": [],
      "source": [
        "def read_images(ids):\n",
        "  images = []\n",
        "  for id in ids:\n",
        "    path = os.path.join(img_path, id + '.jpg')\n",
        "    img = cv2.imread(path)\n",
        "    x_min, y_min, x_max, y_max = bbox[id]\n",
        "    img = img[y_min:y_max, x_min:x_max]\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    images.append(img)\n",
        "  return np.array(images)\n",
        "\n",
        "X_train = read_images(train_id)\n",
        "X_val = read_images(val_id)\n",
        "X_test = read_images(test_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q97DXW2tKfc",
        "outputId": "6b1c9ff6-5f6d-4620-99a4-f2e71a1ba1bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3334, 240, 240, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhkaYp21RDha"
      },
      "source": [
        "2. Construct label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvzqvLEiTaE1"
      },
      "outputs": [],
      "source": [
        "def getFamilies(filename):\n",
        "  families = {}\n",
        "  f = open(os.path.join(data_path, filename), 'r', encoding='utf-8')\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    first_space = line.find(' ')\n",
        "    id = line[:first_space]\n",
        "    family = line[(first_space+1):].replace('\\n', '')\n",
        "    families[id] = family\n",
        "  f.close()\n",
        "  return families\n",
        "\n",
        "train_families = getFamilies('images_family_train.txt')\n",
        "val_families = getFamilies('images_family_val.txt')\n",
        "test_families = getFamilies('images_family_test.txt')\n",
        "\n",
        "# Number of families in train-valid-test set: 70 - 70 - 70"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORGPXxxEXyhh"
      },
      "outputs": [],
      "source": [
        "def getVariants(filename):\n",
        "  variants = {}\n",
        "  f = open(os.path.join(data_path, filename), 'r', encoding='utf-8')\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    first_space = line.find(' ')\n",
        "    id = line[:first_space]\n",
        "    variant = line[(first_space+1):].replace('\\n', '')\n",
        "    variants[id] = variant\n",
        "  f.close()\n",
        "  return variants\n",
        "\n",
        "train_variants = getVariants('images_variant_train.txt')\n",
        "val_variants = getVariants('images_variant_val.txt')\n",
        "test_variants = getVariants('images_variant_test.txt')\n",
        "\n",
        "# Number of variants in train-valid-test set: 100 - 100 - 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vze_PJSTYwR7"
      },
      "outputs": [],
      "source": [
        "def getManufacturers(filename):\n",
        "  manufacturers = {}\n",
        "  f = open(os.path.join(data_path, filename), 'r', encoding='utf-8')\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    first_space = line.find(' ')\n",
        "    id = line[:first_space]\n",
        "    manufacturer = line[(first_space+1):].replace('\\n', '')\n",
        "    manufacturers[id] = manufacturer\n",
        "  f.close()\n",
        "  return manufacturers\n",
        "\n",
        "train_manufacturers = getManufacturers('images_manufacturer_train.txt')\n",
        "val_manufacturers = getManufacturers('images_manufacturer_val.txt')\n",
        "test_manufacturers = getManufacturers('images_manufacturer_test.txt')\n",
        "\n",
        "# Number of manufacturers in train-valid-test set: 30 - 30 - 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtLLeDqqzaiF",
        "outputId": "db847678-06a1-46c0-fde6-5daa1484efdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train labels size: (3334, 3)\n",
            "Valid labels size: (3333, 3)\n",
            "Test labels size: (3333, 3)\n"
          ]
        }
      ],
      "source": [
        "def labels(ids, families, variants, manufacturers):\n",
        "  f = []\n",
        "  v = []\n",
        "  m = []\n",
        "\n",
        "  for id in ids:\n",
        "    f.append(families[id])\n",
        "    v.append(variants[id])\n",
        "    m.append(manufacturers[id])\n",
        "\n",
        "  df = pd.DataFrame({'Family': f, 'Variant': v, 'Manufacturer': m})\n",
        "  return df\n",
        "\n",
        "y_train = labels(train_id, train_families, train_variants, train_manufacturers)\n",
        "y_val = labels(val_id, val_families, val_variants, val_manufacturers)\n",
        "y_test = labels(test_id, test_families, test_variants, test_manufacturers)\n",
        "\n",
        "print('Train labels size:', y_train.shape)\n",
        "print('Valid labels size:', y_val.shape)\n",
        "print('Test labels size:', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMHWOvh1RCzN",
        "outputId": "c3ab7fae-c41f-4f32-db49-bd8cf0bbcae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of families: 70\n",
            "Number of variants: 100\n",
            "Number of manufacturers: 30\n"
          ]
        }
      ],
      "source": [
        "# Label encoder\n",
        "def encoding_map(filename):\n",
        "  mEncoder = {}\n",
        "  i = 0\n",
        "  f = open(os.path.join(data_path, filename), 'r', encoding='utf-8')\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    mEncoder[line.replace('\\n', '')] = int(i)\n",
        "    i += 1\n",
        "  f.close()\n",
        "  return mEncoder\n",
        "\n",
        "family_encoder = encoding_map('families.txt')\n",
        "variant_encoder = encoding_map('variants.txt')\n",
        "manufacturer_encoder = encoding_map('manufacturers.txt')\n",
        "\n",
        "print('Number of families:', len(family_encoder))\n",
        "print('Number of variants:', len(variant_encoder))\n",
        "print('Number of manufacturers:', len(manufacturer_encoder))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z97pnWw4khZC",
        "outputId": "2ec3ce2c-8e21-41c9-f8b1-f439303186ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'707-320': 0,\n",
              " '727-200': 1,\n",
              " '737-200': 2,\n",
              " '737-300': 3,\n",
              " '737-400': 4,\n",
              " '737-500': 5,\n",
              " '737-600': 6,\n",
              " '737-700': 7,\n",
              " '737-800': 8,\n",
              " '737-900': 9,\n",
              " '747-100': 10,\n",
              " '747-200': 11,\n",
              " '747-300': 12,\n",
              " '747-400': 13,\n",
              " '757-200': 14,\n",
              " '757-300': 15,\n",
              " '767-200': 16,\n",
              " '767-300': 17,\n",
              " '767-400': 18,\n",
              " '777-200': 19,\n",
              " '777-300': 20,\n",
              " 'A300B4': 21,\n",
              " 'A310': 22,\n",
              " 'A318': 23,\n",
              " 'A319': 24,\n",
              " 'A320': 25,\n",
              " 'A321': 26,\n",
              " 'A330-200': 27,\n",
              " 'A330-300': 28,\n",
              " 'A340-200': 29,\n",
              " 'A340-300': 30,\n",
              " 'A340-500': 31,\n",
              " 'A340-600': 32,\n",
              " 'A380': 33,\n",
              " 'ATR-42': 34,\n",
              " 'ATR-72': 35,\n",
              " 'An-12': 36,\n",
              " 'BAE 146-200': 37,\n",
              " 'BAE 146-300': 38,\n",
              " 'BAE-125': 39,\n",
              " 'Beechcraft 1900': 40,\n",
              " 'Boeing 717': 41,\n",
              " 'C-130': 42,\n",
              " 'C-47': 43,\n",
              " 'CRJ-200': 44,\n",
              " 'CRJ-700': 45,\n",
              " 'CRJ-900': 46,\n",
              " 'Cessna 172': 47,\n",
              " 'Cessna 208': 48,\n",
              " 'Cessna 525': 49,\n",
              " 'Cessna 560': 50,\n",
              " 'Challenger 600': 51,\n",
              " 'DC-10': 52,\n",
              " 'DC-3': 53,\n",
              " 'DC-6': 54,\n",
              " 'DC-8': 55,\n",
              " 'DC-9-30': 56,\n",
              " 'DH-82': 57,\n",
              " 'DHC-1': 58,\n",
              " 'DHC-6': 59,\n",
              " 'DHC-8-100': 60,\n",
              " 'DHC-8-300': 61,\n",
              " 'DR-400': 62,\n",
              " 'Dornier 328': 63,\n",
              " 'E-170': 64,\n",
              " 'E-190': 65,\n",
              " 'E-195': 66,\n",
              " 'EMB-120': 67,\n",
              " 'ERJ 135': 68,\n",
              " 'ERJ 145': 69,\n",
              " 'Embraer Legacy 600': 70,\n",
              " 'Eurofighter Typhoon': 71,\n",
              " 'F-16A/B': 72,\n",
              " 'F/A-18': 73,\n",
              " 'Falcon 2000': 74,\n",
              " 'Falcon 900': 75,\n",
              " 'Fokker 100': 76,\n",
              " 'Fokker 50': 77,\n",
              " 'Fokker 70': 78,\n",
              " 'Global Express': 79,\n",
              " 'Gulfstream IV': 80,\n",
              " 'Gulfstream V': 81,\n",
              " 'Hawk T1': 82,\n",
              " 'Il-76': 83,\n",
              " 'L-1011': 84,\n",
              " 'MD-11': 85,\n",
              " 'MD-80': 86,\n",
              " 'MD-87': 87,\n",
              " 'MD-90': 88,\n",
              " 'Metroliner': 89,\n",
              " 'Model B200': 90,\n",
              " 'PA-28': 91,\n",
              " 'SR-20': 92,\n",
              " 'Saab 2000': 93,\n",
              " 'Saab 340': 94,\n",
              " 'Spitfire': 95,\n",
              " 'Tornado': 96,\n",
              " 'Tu-134': 97,\n",
              " 'Tu-154': 98,\n",
              " 'Yak-42': 99}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "variant_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tccoh_e5DMBM",
        "outputId": "600d862a-0b08-49ea-a4f4-f9f3661f8f69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boeing :  733\n",
            "Airbus :  434\n",
            "ATR :  66\n",
            "Antonov :  34\n",
            "British Aerospace :  133\n",
            "Beechcraft :  67\n",
            "Lockheed Corporation :  68\n",
            "Douglas Aircraft Company :  133\n",
            "Canadair :  134\n",
            "Cessna :  133\n",
            "McDonnell Douglas :  232\n",
            "de Havilland :  167\n",
            "Robin :  33\n",
            "Dornier :  34\n",
            "Embraer :  233\n",
            "Eurofighter :  33\n",
            "Lockheed Martin :  34\n",
            "Dassault Aviation :  67\n",
            "Fokker :  100\n",
            "Bombardier Aerospace :  33\n",
            "Gulfstream Aerospace :  67\n",
            "Ilyushin :  33\n",
            "Fairchild :  33\n",
            "Piper :  33\n",
            "Cirrus Aircraft :  33\n",
            "Saab :  67\n",
            "Supermarine :  33\n",
            "Panavia :  34\n",
            "Tupolev :  66\n",
            "Yakovlev :  34\n"
          ]
        }
      ],
      "source": [
        "# l_m=list(train_manufacturers.keys())\n",
        "\n",
        "# # =  list(train_manufacturers.values())\n",
        "# train_manufacturers[train_id[0]]\n",
        "# train_variants[train_id[0]]\n",
        "# my_d={}\n",
        "def CountFrequency(my_list):\n",
        "     \n",
        "    # Creating an empty dictionary\n",
        "    freq = {}\n",
        "    for items in my_list:\n",
        "        freq[items] = my_list.count(items)\n",
        "     \n",
        "    for key, value in freq.items():\n",
        "        print (\"% s : % d\"%(key, value))\n",
        "l_m=list(train_manufacturers.values())\n",
        "l_v=list(train_variants.values())\n",
        "\n",
        "CountFrequency(l_m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "495tF2uzO6g4",
        "outputId": "810327cf-34b5-475d-bf17-8cc2c0bab76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Boeing': ['707-320', '727-200', '737-200', '737-300', '737-400', '737-500', '737-600', '737-700', '737-800', '737-900', '747-100', '747-200', '747-300', '747-400', '757-200', '757-300', '767-200', '767-300', '767-400', '777-200', '777-300', 'Boeing 717'], 'Airbus': ['A300B4', 'A310', 'A318', 'A319', 'A320', 'A321', 'A330-200', 'A330-300', 'A340-200', 'A340-300', 'A340-500', 'A340-600', 'A380'], 'ATR': ['ATR-42', 'ATR-72'], 'Antonov': ['An-12'], 'British Aerospace': ['BAE 146-200', 'BAE 146-300', 'BAE-125', 'Hawk T1'], 'Beechcraft': ['Beechcraft 1900', 'Model B200'], 'Lockheed Corporation': ['C-130', 'L-1011'], 'Douglas Aircraft Company': ['C-47', 'DC-3', 'DC-6', 'DC-8'], 'Canadair': ['CRJ-200', 'CRJ-700', 'CRJ-900', 'Challenger 600'], 'Cessna': ['Cessna 172', 'Cessna 208', 'Cessna 525', 'Cessna 560'], 'McDonnell Douglas': ['DC-10', 'DC-9-30', 'F/A-18', 'MD-11', 'MD-80', 'MD-87', 'MD-90'], 'de Havilland': ['DH-82', 'DHC-1', 'DHC-6', 'DHC-8-100', 'DHC-8-300'], 'Robin': ['DR-400'], 'Dornier': ['Dornier 328'], 'Embraer': ['E-170', 'E-190', 'E-195', 'EMB-120', 'ERJ 135', 'ERJ 145', 'Embraer Legacy 600'], 'Eurofighter': ['Eurofighter Typhoon'], 'Lockheed Martin': ['F-16A/B'], 'Dassault Aviation': ['Falcon 2000', 'Falcon 900'], 'Fokker': ['Fokker 100', 'Fokker 50', 'Fokker 70'], 'Bombardier Aerospace': ['Global Express'], 'Gulfstream Aerospace': ['Gulfstream IV', 'Gulfstream V'], 'Ilyushin': ['Il-76'], 'Fairchild': ['Metroliner'], 'Piper': ['PA-28'], 'Cirrus Aircraft': ['SR-20'], 'Saab': ['Saab 2000', 'Saab 340'], 'Supermarine': ['Spitfire'], 'Panavia': ['Tornado'], 'Tupolev': ['Tu-134', 'Tu-154'], 'Yakovlev': ['Yak-42']}\n"
          ]
        }
      ],
      "source": [
        "l_m=list(train_manufacturers.values())\n",
        "l_v=list(train_variants.values())\n",
        "\n",
        "# =  list(train_manufacturers.values())\n",
        "\n",
        "dict_lv={}\n",
        "for i in range(len(l_m)):\n",
        "  if l_m[i] not in dict_lv.keys():\n",
        "    dict_lv[l_m[i]]=[l_v[i]]\n",
        "  elif l_v[i] not in dict_lv[l_m[i]]:\n",
        "    dict_lv[l_m[i]].append(l_v[i])\n",
        "print(dict_lv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsFdr8CCS2qL",
        "outputId": "2893506a-ba36-4c24-903c-422b727be111"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[22,\n",
              " 13,\n",
              " 2,\n",
              " 1,\n",
              " 4,\n",
              " 2,\n",
              " 2,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 7,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "label_lv=[]\n",
        "list_dict= list( dict_lv.keys())\n",
        "for i in range(len(list_dict )):\n",
        "  label_lv.append(len(dict_lv[list_dict[i]]))\n",
        "len(label_lv)\n",
        "label_lv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YphBE_gik91P",
        "outputId": "214b854c-6a8a-4c53-a91f-7cc4ad43dd83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "list_dict= list( dict_lv.keys())\n",
        "\n",
        "LABEL=[[variant_encoder.get(dict_lv[list_dict[i]][j]) for j in range(len(dict_lv[list_dict[i]]))] for i in range(len(list_dict ))]\n",
        "\n",
        "len(LABEL)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXJ_rbR6G8cy",
        "outputId": "29c7d450-0ea1-4685-8844-02730093db44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3333, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I4nPh88ZmfZ",
        "outputId": "a8588ed8-1255-4be0-9de2-ddbe951493e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train labels size: (3334, 3)\n",
            "Valid labels size: (3333, 3)\n",
            "Test labels size: (3333, 3)\n"
          ]
        }
      ],
      "source": [
        "def numerical_labels(ids, families, variants, manufacturers):\n",
        "  f = []\n",
        "  v = []\n",
        "  m = []\n",
        "\n",
        "  for id in ids:\n",
        "    f.append(family_encoder[families[id]])\n",
        "    v.append(variant_encoder[variants[id]])\n",
        "    m.append(manufacturer_encoder[manufacturers[id]])\n",
        "\n",
        "  df = pd.DataFrame({'Family': f, 'Variant': v, 'Manufacturer': m})\n",
        "  return df\n",
        "\n",
        "pos_y_train = numerical_labels(train_id, train_families, train_variants, train_manufacturers)\n",
        "pos_y_val = numerical_labels(val_id, val_families, val_variants, val_manufacturers)\n",
        "pos_y_test = numerical_labels(test_id, test_families, test_variants, test_manufacturers)\n",
        "pos_a_train = numerical_labels(train_id, train_families, train_variants, train_manufacturers)\n",
        "pos_a_val = numerical_labels(val_id, val_families, val_variants, val_manufacturers)\n",
        "pos_a_test = numerical_labels(test_id, test_families, test_variants, test_manufacturers)\n",
        "\n",
        "print('Train labels size:', pos_y_train.shape)\n",
        "print('Valid labels size:', pos_y_val.shape)\n",
        "print('Test labels size:', pos_y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ninpmeJsc96d"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# def OneHotEncoder(label):\n",
        "#   f = to_categorical(label['Family'], num_classes=family_class, dtype='int')\n",
        "#   # f = to_categorical(label['Family'], num_classes=variant_class, dtype='int') \n",
        "#   v = to_categorical(label['Variant'], num_classes=variant_class, dtype='int')             \n",
        "#   # m = to_categorical(label['Manufacturer'], num_classes=manufacturer_class, dtype='int')\n",
        "#   m = to_categorical(label['Manufacturer'], num_classes=variant_class, dtype='int')\n",
        "\n",
        "#   lf = f.tolist()\n",
        "#   lv = v.tolist()\n",
        "#   lm = m.tolist()\n",
        "\n",
        "#   arr = []\n",
        "#   for i in range(label.shape[0]):\n",
        "#     tmp = []\n",
        "#     tmp.append(lf[i])\n",
        "#     tmp.append(lv[i])\n",
        "#     tmp.append(lm[i])\n",
        "#     arr.append(tmp)\n",
        "\n",
        "#   label['Family'] = lf\n",
        "#   label['Variant'] = lv\n",
        "#   label['Manufacturer'] = lm\n",
        "  \n",
        "#   # return label\n",
        "#   return np.array(arr)\n",
        "\n",
        "# y_train = OneHotEncoder(pos_y_train)\n",
        "# y_val = OneHotEncoder(pos_y_val)\n",
        "# y_test = OneHotEncoder(pos_y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elKQDZ51bNO7"
      },
      "outputs": [],
      "source": [
        "family_class = 70\n",
        "variant_class = 100\n",
        "manufacturer_class = 30\n",
        "\n",
        "# One-hot encoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def OneHotEncoder(label):\n",
        "  f = to_categorical(label['Family'], num_classes=family_class, dtype='int')\n",
        "  v = to_categorical(label['Variant'], num_classes=variant_class, dtype='int')             \n",
        "  m = to_categorical(label['Manufacturer'], num_classes=manufacturer_class, dtype='int')\n",
        "\n",
        "  lf = f.tolist()\n",
        "  lv = v.tolist()\n",
        "  lm = m.tolist()\n",
        "\n",
        "  label['Family'] = lf\n",
        "  label['Variant'] = lv\n",
        "  label['Manufacturer'] = lm\n",
        "  \n",
        "  return label\n",
        "\n",
        "y_train = OneHotEncoder(pos_y_train)\n",
        "y_val = OneHotEncoder(pos_y_val)\n",
        "y_test = OneHotEncoder(pos_y_test)\n",
        "\n",
        "a_train = OneHotEncoder(pos_a_train)\n",
        "a_val = OneHotEncoder(pos_a_val)\n",
        "a_test = OneHotEncoder(pos_a_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dV3RTf1V5VU"
      },
      "outputs": [],
      "source": [
        "y_train.drop(columns=['Manufacturer','Family'], inplace=True)\n",
        "y_val.drop(columns=[ 'Manufacturer','Family'], inplace=True)\n",
        "y_test.drop(columns=[ 'Manufacturer','Family'], inplace=True)\n",
        "\n",
        "# np.array(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ykp_sm2kcHKM"
      },
      "outputs": [],
      "source": [
        "a_train.drop(columns=['Family','Variant'], inplace=True)\n",
        "a_val.drop(columns=['Family','Variant'], inplace=True)\n",
        "a_test.drop(columns=['Family','Variant'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOH-u95nxdju"
      },
      "outputs": [],
      "source": [
        "\n",
        "def convert_NP(label):\n",
        "  arr = []\n",
        "  for i in range(label['Variant'].shape[0]):\n",
        "    arr.append(label['Variant'].loc[i])\n",
        "  arr = np.array(arr)\n",
        "  return arr\n",
        "y_train= convert_NP(y_train)\n",
        "y_val= convert_NP(y_val)\n",
        "y_test= convert_NP(y_test)\n",
        "\n",
        "# test= convert_NP(a_train)\n",
        "# a_val= convert_NP(a_val)\n",
        "# a_test= convert_NP(a_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4RtftMZ-kG9"
      },
      "outputs": [],
      "source": [
        "def convert_NP(label):\n",
        "  arr = []\n",
        "  for i in range(label['Manufacturer'].shape[0]):\n",
        "    arr.append(label['Manufacturer'].loc[i])\n",
        "  arr = np.array(arr)\n",
        "  return arr\n",
        "a_train= convert_NP(a_train)\n",
        "a_val= convert_NP(a_val)\n",
        "a_test= convert_NP(a_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOzSY9xxcF0m"
      },
      "outputs": [],
      "source": [
        "m_pred=np.array([])\n",
        "\n",
        "for num in label_lv:\n",
        "  if m_pred.shape[0]==0:\n",
        "    m_pred=np.sum(y_train[:,:num], axis=1)\n",
        "  else:\n",
        "   m_pred=np.append(m_pred,np.sum(y_train[:,:num],axis=1))\n",
        "   \n",
        "# np.sum(y_train[:,:2], axis=1)\n",
        "# y_train[:,:2]\n",
        "m_pred=m_pred.reshape((30, m_pred.shape[0]//30)).T\n",
        "# m_pred=m_pred.reshape(( m_pred.shape[0]//30,30))\n",
        "\n",
        "\n",
        "test=np.sum(y_train[:,:label_lv[1]], axis=1)\n",
        "\n",
        "a=m_pred[:,1]- test\n",
        "sum(a)\n",
        "# a=np.array([1,2,3,4,5,6])\n",
        "# a.reshape(3,2)\n",
        "\n",
        "a=np.append(a_train[0],np.zeros(70))\n",
        "for i in range(1,a_train.shape[0]):\n",
        "  \n",
        "  a=np.row_stack((a,np.append(a_train[i],np.zeros(70))))\n",
        "\n",
        "b=np.append(a_val[0],np.zeros(70))\n",
        "for i in range(1,a_val.shape[0]):\n",
        "  \n",
        "  b=np.row_stack((b,np.append(a_val[i],np.zeros(70))))\n",
        "\n",
        "x= np.zeros((3334,2,100))\n",
        "for i in range ( y_train.shape[0]):\n",
        "  x[i]= np.row_stack((y_train[i],a[i]))\n",
        "y_train=x\n",
        "\n",
        "y= np.zeros((3333,2,100))\n",
        "for i in range ( y_val.shape[0]):\n",
        "  y[i]= np.row_stack((y_val[i],b[i]))\n",
        "y_val=y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK2TpQd9qX48",
        "outputId": "e2e472db-7a89-4164-bddc-ca5ba4aa6bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3334, 2, 100)\n",
            "(3333, 2, 100)\n"
          ]
        }
      ],
      "source": [
        "print(y_train.shape)\n",
        "print(y_val.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H8RCtTr1Et_"
      },
      "outputs": [],
      "source": [
        "def format_label(label):\n",
        "  def get_key(my_dict, val):\n",
        "    for key, value in my_dict.items():\n",
        "         if val == value:\n",
        "             return key\n",
        " \n",
        "    return \"key doesn't exist\"\n",
        "  \n",
        "  f = get_key(family_encoder, label['Family'])\n",
        "  v = get_key(variant_encoder, label['Variant'])\n",
        "  m = get_key(manufacturer_encoder, label['Manufacturer'])\n",
        "  \n",
        "  # return f\"Family: {f} ; Variant: {v} ; Manufacturer: {m}\"\n",
        "  return f, v, m\n",
        "\n",
        "def decode_label(label):\n",
        "  def get_key(my_dict, val):\n",
        "    for key, value in my_dict.items():\n",
        "         if val == value:\n",
        "             return key\n",
        " \n",
        "    return \"key doesn't exist\"\n",
        "  \n",
        "  f = get_key(family_encoder, np.argmax(label[0]))\n",
        "  v = get_key(variant_encoder, np.argmax(label[1]))\n",
        "  m = get_key(manufacturer_encoder, np.argmax(label[2]))\n",
        "  \n",
        "  return f\"Family: {f} ; Variant: {v} ; Manufacturer: {m}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K56m3P_pvpzg"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzlYgUYCIY2o",
        "outputId": "44871e15-4187-4354-e8cf-6d39c673f6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a TPU runtime. Using CPU/GPU strategy\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    print(\"Device:\", tpu.master())\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
        "    strategy = tf.distribute.MirroredStrategy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHMvKZ_9HWPS",
        "outputId": "1e167ec5-bb96-48e0-b2ff-e76c90bd4159"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xffU_u3KJGoP"
      },
      "source": [
        "# Data Augumentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxAxOG_tKKmH"
      },
      "outputs": [],
      "source": [
        "img_augmentation = Sequential(\n",
        "    [\n",
        "        layers.RandomRotation(factor=0.15),\n",
        "        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
        "        layers.RandomFlip(),\n",
        "        layers.RandomContrast(factor=0.1),\n",
        "    ],\n",
        "    name=\"img_augmentation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnWq_335_Ss0",
        "outputId": "e29cba91-8aa6-4020-a72a-302fd8c753ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# ! git clone https://github.com/yiyixuxu/polyloss-pytorch.git\n",
        "# %cd polyloss-pytorch\n",
        "y_train[:,0,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9rjNa5D_ZIk"
      },
      "outputs": [],
      "source": [
        "# def poly1_cross_entropy(epsilon):\n",
        "#     def _poly1_cross_entropy(y_true, y_pred):\n",
        "#         # pt, CE, and Poly1 have shape [batch].\n",
        "#         pt = tf.reduce_sum(float(y_true) * float(tf.nn.softmax(y_pred)), axis=-1)\n",
        "#         CE = tf.nn.softmax_cross_entropy_with_logits(y_true, y_pred)\n",
        "#         Poly1 = CE + epsilon * (1 - pt)\n",
        "#         #loss = tf.reduce_mean(Poly1)\n",
        "#         return Poly1\n",
        "#     return _poly1_cross_entropy\n",
        "\n",
        "def poly1_cross_entropy(epsilon ,label=LABEL ):\n",
        "    def _poly1_cross_entropy(y_true, y_pred,alpha=0.1):\n",
        "        # pt, CE, and Poly1 have shape [batch].\n",
        "        num_classes = y_true[:,0,:].get_shape().as_list()[-1]\n",
        "        smooth_labels = float(y_true[:,0,:]) * float(1-alpha) + alpha/float(num_classes)\n",
        "        one_minus_pt = tf.reduce_sum(\n",
        "        float(smooth_labels) * float(1 - tf.nn.softmax(y_pred)), axis=-1)\n",
        "        CE_loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "          from_logits=True, label_smoothing=alpha, reduction='none')\n",
        "        CE = CE_loss(y_true[:,0,:], y_pred)\n",
        "        Poly1_V = CE + epsilon * one_minus_pt\n",
        "\n",
        "########################################\n",
        "        m_pred=tf.constant([])\n",
        "        x= y_pred\n",
        "\n",
        "\n",
        "        for i in range(0,len(LABEL)):\n",
        "          if i==0:\n",
        "            m_pred=tf.reduce_sum( [x[:,index]   for index in LABEL[i]],axis=0)\n",
        "          else:\n",
        "            m_pred=tf.concat([m_pred,tf.reduce_sum( [x[:,index]   for index in LABEL[i]],axis=0)],axis=0)\n",
        "\n",
        "        m_pred=tf.reshape(m_pred,[30, -1])\n",
        "        m_pred=tf.transpose(m_pred)\n",
        "\n",
        "        smooth_labels = float(y_true[:,1,:30]) * float(1-alpha) + alpha/float(num_classes)\n",
        "        one_minus_pt = tf.reduce_sum(\n",
        "        float(smooth_labels) * float(1 - tf.nn.softmax(m_pred)), axis=-1)\n",
        "        CE_loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "          from_logits=True, label_smoothing=alpha, reduction='none')\n",
        "        CE = CE_loss(y_true[:,1,:30], m_pred)\n",
        "        Poly1_M = CE + epsilon * one_minus_pt\n",
        "        # Poly1_M=0\n",
        "        #print(y_true.get_shape())\n",
        "        return Poly1_V + 0.5*Poly1_M\n",
        "    return _poly1_cross_entropy\n",
        "\n",
        "# def poly1_cross_entropy(epsilon):\n",
        "#     def _poly1_cross_entropy(y_true, y_pred,alpha=0.1):\n",
        "#         # pt, CE, and Poly1 have shape [batch].\n",
        "#         num_classes = y_true.get_shape().as_list()[-1]\n",
        "#         smooth_labels = float(y_true) * float(1-alpha) + alpha/float(num_classes)\n",
        "#         one_minus_pt = tf.reduce_sum(\n",
        "#         float(smooth_labels) * float(1 - tf.nn.softmax(y_pred)), axis=-1)\n",
        "#         CE_loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "#           from_logits=True, label_smoothing=alpha, reduction='none')\n",
        "#         CE = CE_loss(y_true, y_pred)\n",
        "#         Poly1 = CE + epsilon * one_minus_pt\n",
        "#         print(y_true.get_shape())\n",
        "#         return Poly1\n",
        "#     return _poly1_cross_entropy\n",
        "\n",
        "def poly1_focal_loss(gamma=2.0, epsilon=1.0, alpha=0.25):\n",
        "    def _poly1_focal_loss(y_true, y_pred):\n",
        "        p = tf.math.sigmoid(y_pred)\n",
        "        ce_loss = tf.nn.sigmoid_cross_entropy_with_logits(y_true, y_pred)\n",
        "        pt = y_true * p + (1 - y_true) * (1 - p)\n",
        "        FL = ce_loss * ((1 - pt) ** gamma)\n",
        "        \n",
        "        if alpha >= 0:\n",
        "            alpha_t = alpha * y_true + (1 - alpha) * (1 - y_true)\n",
        "            FL = alpha_t * FL\n",
        "        Poly1 = FL + epsilon * tf.math.pow(1 - pt, gamma + 1)\n",
        "        loss = tf.reduce_mean(Poly1)\n",
        "        return loss\n",
        "    return _poly1_focal_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtmMRS3eIm7X"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "m = tf.keras.metrics.Accuracy()\n",
        "def my_metric_fn(y_true, y_pred):\n",
        "    \n",
        "    # score= 0\n",
        "    # for i in range(y_true.get_shape().as_list()[0]):\n",
        "    #    if tf.math.argmax(y_true[i,0])==tf.math.argmax(y_pred[i]):\n",
        "    #      score+=1\n",
        "    true=  (tf.math.argmax(y_true[:,0,:], axis=1)+1)\n",
        "    pred=  (tf.math.argmax(y_pred, axis=1)+1)\n",
        "    \n",
        "    m.update_state(true, pred)\n",
        "    \n",
        "    return m.result()\n",
        "      # return tf.cast(tf.math.equal(y_true[:,0,:], y_pred), tf.float32)\n",
        "\n",
        "# accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=my_metric_fn)\n",
        "def kullback_leibler_divergence(y_true, y_pred):\n",
        "    Y= y_true[:,0,:]\n",
        "    y_true[:,0,:].assign(K.clip(Y, K.epsilon(), 1))\n",
        "    y_pred = K.clip(Y, K.epsilon(), 1)\n",
        "    return K.sum(Y * K.log(Y / y_pred), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s8aRpi6Jje6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b87ccff-7cbe-4d9b-f6a4-c6d2572583a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]], shape=(3334, 100), dtype=float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0,\n",
              "  1,\n",
              "  2,\n",
              "  3,\n",
              "  4,\n",
              "  5,\n",
              "  6,\n",
              "  7,\n",
              "  8,\n",
              "  9,\n",
              "  10,\n",
              "  11,\n",
              "  12,\n",
              "  13,\n",
              "  14,\n",
              "  15,\n",
              "  16,\n",
              "  17,\n",
              "  18,\n",
              "  19,\n",
              "  20,\n",
              "  41],\n",
              " [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33],\n",
              " [34, 35],\n",
              " [36],\n",
              " [37, 38, 39, 82],\n",
              " [40, 90],\n",
              " [42, 84],\n",
              " [43, 53, 54, 55],\n",
              " [44, 45, 46, 51],\n",
              " [47, 48, 49, 50],\n",
              " [52, 56, 73, 85, 86, 87, 88],\n",
              " [57, 58, 59, 60, 61],\n",
              " [62],\n",
              " [63],\n",
              " [64, 65, 66, 67, 68, 69, 70],\n",
              " [71],\n",
              " [72],\n",
              " [74, 75],\n",
              " [76, 77, 78],\n",
              " [79],\n",
              " [80, 81],\n",
              " [83],\n",
              " [89],\n",
              " [91],\n",
              " [92],\n",
              " [93, 94],\n",
              " [95],\n",
              " [96],\n",
              " [97, 98],\n",
              " [99]]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "x= tf.convert_to_tensor(y_train[:,0,:])\n",
        "print(x)\n",
        "      \n",
        "a_pred=tf.constant([])\n",
        "\n",
        "for i in range(0,len(LABEL)):\n",
        "  if i==0:\n",
        "    a_pred=tf.reduce_sum( [x[:,index]   for index in LABEL[i]],axis=0)\n",
        "  else:\n",
        "    a_pred=tf.concat([a_pred,tf.reduce_sum( [x[:,index]  for index in LABEL[i]],axis=0)],axis=0)\n",
        "  #print(a_pred)\n",
        "\n",
        "a_pred=tf.reshape(a_pred,[30, -1])\n",
        "a_pred=tf.transpose(a_pred)\n",
        "a_pred\n",
        "test=np.sum(y_train[:,0,LABEL[2]], axis=1)\n",
        "\n",
        "a=a_pred[:,2]- test\n",
        "\n",
        "sum(a)\n",
        "LABEL# if tf.math.argmax(x[0,0]) == tf.math.argmax(x[0,0]):\n",
        "#   print(0)\n",
        "# xr = (tf.math.argmax(x[:,0,:], axis=1)+1)\n",
        "# # a= tf.convert_to_tensor(0,1) \n",
        "# # a\n",
        "# m = tf.keras.metrics.Accuracy()\n",
        "# m.update_state(xr, xr)\n",
        "# m.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2wtKi5XdbDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d07ebc-5513-419b-c11c-2aa58bef14d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0,\n",
              "  1,\n",
              "  2,\n",
              "  3,\n",
              "  4,\n",
              "  5,\n",
              "  6,\n",
              "  7,\n",
              "  8,\n",
              "  9,\n",
              "  10,\n",
              "  11,\n",
              "  12,\n",
              "  13,\n",
              "  14,\n",
              "  15,\n",
              "  16,\n",
              "  17,\n",
              "  18,\n",
              "  19,\n",
              "  20,\n",
              "  41],\n",
              " [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33],\n",
              " [34, 35],\n",
              " [36],\n",
              " [37, 38, 39, 82],\n",
              " [40, 90],\n",
              " [42, 84],\n",
              " [43, 53, 54, 55],\n",
              " [44, 45, 46, 51],\n",
              " [47, 48, 49, 50],\n",
              " [52, 56, 73, 85, 86, 87, 88],\n",
              " [57, 58, 59, 60, 61],\n",
              " [62],\n",
              " [63],\n",
              " [64, 65, 66, 67, 68, 69, 70],\n",
              " [71],\n",
              " [72],\n",
              " [74, 75],\n",
              " [76, 77, 78],\n",
              " [79],\n",
              " [80, 81],\n",
              " [83],\n",
              " [89],\n",
              " [91],\n",
              " [92],\n",
              " [93, 94],\n",
              " [95],\n",
              " [96],\n",
              " [97, 98],\n",
              " [99]]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "LABEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukNyJ6HiJMHu"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEo-BGhpJbcW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import EfficientNetB1\n",
        "\n",
        "def build_model(num_classes):\n",
        "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    x = img_augmentation(inputs)\n",
        "    #x = inputs\n",
        "    model = EfficientNetB1(include_top=False, input_tensor=x, weights='imagenet')\n",
        "\n",
        "    # Freeze the pretrained weights ImageNet\n",
        "    #model.trainable = False\n",
        "\n",
        "    # Rebuild top\n",
        "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    top_dropout_rate = 0.2\n",
        "    print(x)\n",
        "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "    # x = layers.Dense(200, activation='relu', name='dense_1')(x)\n",
        "    # x = layers.Reshape((2,100))(x)\n",
        "    # outputs= softmax(x,axis=2)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n",
        "    print(x)\n",
        "    print(outputs.shape)\n",
        "\n",
        "    # Compile\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    model.compile(\n",
        "        #optimizer=optimizer, loss=poly1_cross_entropy(epsilon=0), metrics=[\"accuracy\"]\n",
        "        #optimizer=optimizer, loss=poly1_cross_entropy(epsilon=1,label=LABEL)\n",
        "         optimizer=optimizer, loss=kullback_leibler_divergence\n",
        "        #, metrics=[my_metric_fn]                                                    ############## ACC\n",
        "        #optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"]\n",
        "    )\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiUs8cfC3RQi"
      },
      "outputs": [],
      "source": [
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "                                filepath='best_b1_poly_e=2.h5',\n",
        "                                save_weights_only=True,\n",
        "                                monitor='val_loss',\n",
        "                                mode='min',\n",
        "                                save_best_only=True,\n",
        "                                verbose=1\n",
        "                            )\n",
        "model_early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, min_delta=0, verbose=0, mode='min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "R-3o87E9RNkr",
        "outputId": "220cc886-a27e-45df-8460-26df1bfdd8e1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-8cf9e7a44bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# del model\n",
        "\n",
        "# K.clear_session()\n",
        "# gc.collect()\n",
        "# del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NTa_G7_h4o6",
        "outputId": "8b2a211a-342d-421b-e733-f515eab8f5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1280), dtype=tf.float32, name=None), name='batch_normalization/batchnorm/add_1:0', description=\"created by layer 'batch_normalization'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1280), dtype=tf.float32, name=None), name='top_dropout/Identity:0', description=\"created by layer 'top_dropout'\")\n",
            "(None, 100)\n",
            "Epoch 1/60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 6.98535, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 101s - loss: 7.7230 - val_loss: 6.9853 - 101s/epoch - 2s/step\n",
            "Epoch 2/60\n",
            "\n",
            "Epoch 2: val_loss improved from 6.98535 to 6.47359, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 61s - loss: 6.1080 - val_loss: 6.4736 - 61s/epoch - 1s/step\n",
            "Epoch 3/60\n",
            "\n",
            "Epoch 3: val_loss improved from 6.47359 to 5.86317, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 62s - loss: 5.4326 - val_loss: 5.8632 - 62s/epoch - 1s/step\n",
            "Epoch 4/60\n",
            "\n",
            "Epoch 4: val_loss improved from 5.86317 to 5.61311, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 62s - loss: 4.9913 - val_loss: 5.6131 - 62s/epoch - 1s/step\n",
            "Epoch 5/60\n",
            "\n",
            "Epoch 5: val_loss improved from 5.61311 to 5.43842, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.7850 - val_loss: 5.4384 - 63s/epoch - 1s/step\n",
            "Epoch 6/60\n",
            "\n",
            "Epoch 6: val_loss improved from 5.43842 to 5.35058, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.5970 - val_loss: 5.3506 - 63s/epoch - 1s/step\n",
            "Epoch 7/60\n",
            "\n",
            "Epoch 7: val_loss improved from 5.35058 to 5.29700, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.5297 - val_loss: 5.2970 - 63s/epoch - 1s/step\n",
            "Epoch 8/60\n",
            "\n",
            "Epoch 8: val_loss improved from 5.29700 to 5.29135, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.3884 - val_loss: 5.2913 - 63s/epoch - 1s/step\n",
            "Epoch 9/60\n",
            "\n",
            "Epoch 9: val_loss improved from 5.29135 to 5.23141, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.3592 - val_loss: 5.2314 - 63s/epoch - 1s/step\n",
            "Epoch 10/60\n",
            "\n",
            "Epoch 10: val_loss did not improve from 5.23141\n",
            "53/53 - 62s - loss: 4.3146 - val_loss: 5.2748 - 62s/epoch - 1s/step\n",
            "Epoch 11/60\n",
            "\n",
            "Epoch 11: val_loss did not improve from 5.23141\n",
            "53/53 - 62s - loss: 4.2881 - val_loss: 5.2597 - 62s/epoch - 1s/step\n",
            "Epoch 12/60\n",
            "\n",
            "Epoch 12: val_loss improved from 5.23141 to 5.16699, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.2573 - val_loss: 5.1670 - 63s/epoch - 1s/step\n",
            "Epoch 13/60\n",
            "\n",
            "Epoch 13: val_loss improved from 5.16699 to 5.14534, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.2159 - val_loss: 5.1453 - 63s/epoch - 1s/step\n",
            "Epoch 14/60\n",
            "\n",
            "Epoch 14: val_loss improved from 5.14534 to 5.10573, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.2205 - val_loss: 5.1057 - 63s/epoch - 1s/step\n",
            "Epoch 15/60\n",
            "\n",
            "Epoch 15: val_loss improved from 5.10573 to 5.07891, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.1887 - val_loss: 5.0789 - 63s/epoch - 1s/step\n",
            "Epoch 16/60\n",
            "\n",
            "Epoch 16: val_loss did not improve from 5.07891\n",
            "53/53 - 62s - loss: 4.1867 - val_loss: 5.1701 - 62s/epoch - 1s/step\n",
            "Epoch 17/60\n",
            "\n",
            "Epoch 17: val_loss improved from 5.07891 to 5.07671, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.1334 - val_loss: 5.0767 - 63s/epoch - 1s/step\n",
            "Epoch 18/60\n",
            "\n",
            "Epoch 18: val_loss did not improve from 5.07671\n",
            "53/53 - 62s - loss: 4.1288 - val_loss: 5.1142 - 62s/epoch - 1s/step\n",
            "Epoch 19/60\n",
            "\n",
            "Epoch 19: val_loss did not improve from 5.07671\n",
            "53/53 - 62s - loss: 4.1311 - val_loss: 5.0904 - 62s/epoch - 1s/step\n",
            "Epoch 20/60\n",
            "\n",
            "Epoch 20: val_loss did not improve from 5.07671\n",
            "53/53 - 62s - loss: 4.1336 - val_loss: 5.1341 - 62s/epoch - 1s/step\n",
            "Epoch 21/60\n",
            "\n",
            "Epoch 21: val_loss did not improve from 5.07671\n",
            "53/53 - 62s - loss: 4.1414 - val_loss: 5.0940 - 62s/epoch - 1s/step\n",
            "Epoch 22/60\n",
            "\n",
            "Epoch 22: val_loss improved from 5.07671 to 5.07210, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.1662 - val_loss: 5.0721 - 63s/epoch - 1s/step\n",
            "Epoch 23/60\n",
            "\n",
            "Epoch 23: val_loss did not improve from 5.07210\n",
            "53/53 - 62s - loss: 4.1413 - val_loss: 5.0729 - 62s/epoch - 1s/step\n",
            "Epoch 24/60\n",
            "\n",
            "Epoch 24: val_loss did not improve from 5.07210\n",
            "53/53 - 62s - loss: 4.1401 - val_loss: 5.0965 - 62s/epoch - 1s/step\n",
            "Epoch 25/60\n",
            "\n",
            "Epoch 25: val_loss did not improve from 5.07210\n",
            "53/53 - 62s - loss: 4.1458 - val_loss: 5.1052 - 62s/epoch - 1s/step\n",
            "Epoch 26/60\n",
            "\n",
            "Epoch 26: val_loss did not improve from 5.07210\n",
            "53/53 - 62s - loss: 4.0803 - val_loss: 5.0763 - 62s/epoch - 1s/step\n",
            "Epoch 27/60\n",
            "\n",
            "Epoch 27: val_loss improved from 5.07210 to 4.97693, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.1008 - val_loss: 4.9769 - 63s/epoch - 1s/step\n",
            "Epoch 28/60\n",
            "\n",
            "Epoch 28: val_loss did not improve from 4.97693\n",
            "53/53 - 62s - loss: 4.0979 - val_loss: 4.9903 - 62s/epoch - 1s/step\n",
            "Epoch 29/60\n",
            "\n",
            "Epoch 29: val_loss improved from 4.97693 to 4.96578, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.0746 - val_loss: 4.9658 - 63s/epoch - 1s/step\n",
            "Epoch 30/60\n",
            "\n",
            "Epoch 30: val_loss improved from 4.96578 to 4.94785, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.0559 - val_loss: 4.9479 - 63s/epoch - 1s/step\n",
            "Epoch 31/60\n",
            "\n",
            "Epoch 31: val_loss improved from 4.94785 to 4.92944, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.0534 - val_loss: 4.9294 - 63s/epoch - 1s/step\n",
            "Epoch 32/60\n",
            "\n",
            "Epoch 32: val_loss did not improve from 4.92944\n",
            "53/53 - 62s - loss: 4.0594 - val_loss: 4.9736 - 62s/epoch - 1s/step\n",
            "Epoch 33/60\n",
            "\n",
            "Epoch 33: val_loss did not improve from 4.92944\n",
            "53/53 - 62s - loss: 4.0648 - val_loss: 5.0416 - 62s/epoch - 1s/step\n",
            "Epoch 34/60\n",
            "\n",
            "Epoch 34: val_loss did not improve from 4.92944\n",
            "53/53 - 62s - loss: 4.0597 - val_loss: 4.9540 - 62s/epoch - 1s/step\n",
            "Epoch 35/60\n",
            "\n",
            "Epoch 35: val_loss did not improve from 4.92944\n",
            "53/53 - 62s - loss: 4.0723 - val_loss: 5.0241 - 62s/epoch - 1s/step\n",
            "Epoch 36/60\n",
            "\n",
            "Epoch 36: val_loss did not improve from 4.92944\n",
            "53/53 - 62s - loss: 4.0546 - val_loss: 5.0015 - 62s/epoch - 1s/step\n",
            "Epoch 37/60\n",
            "\n",
            "Epoch 37: val_loss did not improve from 4.92944\n",
            "53/53 - 62s - loss: 4.0389 - val_loss: 4.9649 - 62s/epoch - 1s/step\n",
            "Epoch 38/60\n",
            "\n",
            "Epoch 38: val_loss did not improve from 4.92944\n",
            "53/53 - 62s - loss: 4.0475 - val_loss: 4.9834 - 62s/epoch - 1s/step\n",
            "Epoch 39/60\n",
            "\n",
            "Epoch 39: val_loss did not improve from 4.92944\n",
            "53/53 - 62s - loss: 4.0706 - val_loss: 5.0409 - 62s/epoch - 1s/step\n",
            "Epoch 40/60\n",
            "\n",
            "Epoch 40: val_loss did not improve from 4.92944\n",
            "53/53 - 62s - loss: 4.0529 - val_loss: 4.9628 - 62s/epoch - 1s/step\n",
            "Epoch 41/60\n",
            "\n",
            "Epoch 41: val_loss improved from 4.92944 to 4.91019, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.0180 - val_loss: 4.9102 - 63s/epoch - 1s/step\n",
            "Epoch 42/60\n",
            "\n",
            "Epoch 42: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0229 - val_loss: 4.9678 - 62s/epoch - 1s/step\n",
            "Epoch 43/60\n",
            "\n",
            "Epoch 43: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0245 - val_loss: 4.9509 - 62s/epoch - 1s/step\n",
            "Epoch 44/60\n",
            "\n",
            "Epoch 44: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0157 - val_loss: 4.9315 - 62s/epoch - 1s/step\n",
            "Epoch 45/60\n",
            "\n",
            "Epoch 45: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0215 - val_loss: 4.9560 - 62s/epoch - 1s/step\n",
            "Epoch 46/60\n",
            "\n",
            "Epoch 46: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0112 - val_loss: 4.9678 - 62s/epoch - 1s/step\n",
            "Epoch 47/60\n",
            "\n",
            "Epoch 47: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0306 - val_loss: 4.9902 - 62s/epoch - 1s/step\n",
            "Epoch 48/60\n",
            "\n",
            "Epoch 48: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0369 - val_loss: 4.9443 - 62s/epoch - 1s/step\n",
            "Epoch 49/60\n",
            "\n",
            "Epoch 49: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0370 - val_loss: 4.9807 - 62s/epoch - 1s/step\n",
            "Epoch 50/60\n",
            "\n",
            "Epoch 50: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0458 - val_loss: 5.0089 - 62s/epoch - 1s/step\n",
            "Epoch 51/60\n",
            "\n",
            "Epoch 51: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0351 - val_loss: 4.9736 - 62s/epoch - 1s/step\n",
            "Epoch 52/60\n",
            "\n",
            "Epoch 52: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0284 - val_loss: 4.9602 - 62s/epoch - 1s/step\n",
            "Epoch 53/60\n",
            "\n",
            "Epoch 53: val_loss did not improve from 4.91019\n",
            "53/53 - 62s - loss: 4.0306 - val_loss: 4.9405 - 62s/epoch - 1s/step\n",
            "Epoch 54/60\n",
            "\n",
            "Epoch 54: val_loss improved from 4.91019 to 4.85517, saving model to best_b1_poly_e=2.h5\n",
            "53/53 - 63s - loss: 4.0010 - val_loss: 4.8552 - 63s/epoch - 1s/step\n",
            "Epoch 55/60\n",
            "\n",
            "Epoch 55: val_loss did not improve from 4.85517\n",
            "53/53 - 62s - loss: 4.0165 - val_loss: 4.9261 - 62s/epoch - 1s/step\n",
            "Epoch 56/60\n",
            "\n",
            "Epoch 56: val_loss did not improve from 4.85517\n",
            "53/53 - 62s - loss: 4.0235 - val_loss: 4.9528 - 62s/epoch - 1s/step\n",
            "Epoch 57/60\n",
            "\n",
            "Epoch 57: val_loss did not improve from 4.85517\n",
            "53/53 - 62s - loss: 4.0117 - val_loss: 4.9251 - 62s/epoch - 1s/step\n",
            "Epoch 58/60\n",
            "\n",
            "Epoch 58: val_loss did not improve from 4.85517\n",
            "53/53 - 62s - loss: 4.0133 - val_loss: 4.9712 - 62s/epoch - 1s/step\n",
            "Epoch 59/60\n",
            "\n",
            "Epoch 59: val_loss did not improve from 4.85517\n",
            "53/53 - 62s - loss: 3.9883 - val_loss: 4.9050 - 62s/epoch - 1s/step\n",
            "Epoch 60/60\n",
            "\n",
            "Epoch 60: val_loss did not improve from 4.85517\n",
            "53/53 - 62s - loss: 4.0081 - val_loss: 4.9190 - 62s/epoch - 1s/step\n"
          ]
        }
      ],
      "source": [
        "#model.summary()\n",
        "\n",
        "\n",
        "\n",
        "############## CHAY O DAY LEN #####################\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "    model = build_model(num_classes=y_val.shape[-1])\n",
        "#model.load_weights('b7_244.h5')\n",
        "\n",
        "epochs = 60\n",
        "hist= model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), verbose=2,\n",
        "               callbacks=[model_checkpoint_callback],\n",
        "                          # , model_early_stopping_callback],\n",
        "                batch_size=64)\n",
        "# model.fit(train_generator, validation_data=val_generator, verbose=2, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "7ZDw1RPwVWTo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "outputId": "3f8043e1-b630-466c-d1c5-93efad6a2344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1280), dtype=tf.float32, name=None), name='batch_normalization_4/batchnorm/add_1:0', description=\"created by layer 'batch_normalization_4'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1280), dtype=tf.float32, name=None), name='top_dropout/Identity:0', description=\"created by layer 'top_dropout'\")\n",
            "(None, 100)\n",
            "Epoch 1/60\n",
            "INFO:tensorflow:Error reported to Coordinator: in user code:\n",
            "\n",
            "    File \"<ipython-input-54-c85a36031b9c>\", line 20, in kullback_leibler_divergence  *\n",
            "        y_true[:,0,:].assign(K.clip(Y, K.epsilon(), 1))\n",
            "\n",
            "    AttributeError: 'Tensor' object has no attribute 'assign'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/coordinator.py\", line 293, in stop_on_exception\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/mirrored_run.py\", line 342, in run\n",
            "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 692, in wrapper\n",
            "    raise e.ag_error_metadata.to_exception(e)\n",
            "AttributeError: in user code:\n",
            "\n",
            "    File \"<ipython-input-54-c85a36031b9c>\", line 20, in kullback_leibler_divergence  *\n",
            "        y_true[:,0,:].assign(K.clip(Y, K.epsilon(), 1))\n",
            "\n",
            "    AttributeError: 'Tensor' object has no attribute 'assign'\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-fd2a29c58220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                           \u001b[0;31m# , model_early_stopping_callback],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 batch_size=64)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# model.fit(train_generator, validation_data=val_generator, verbose=2, epochs=epochs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-54-c85a36031b9c>\", line 20, in kullback_leibler_divergence  *\n        y_true[:,0,:].assign(K.clip(Y, K.epsilon(), 1))\n\n    AttributeError: 'Tensor' object has no attribute 'assign'\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    model = build_model(num_classes=y_val.shape[-1])\n",
        "#model.load_weights('b7_244.h5')\n",
        "\n",
        "epochs = 60  # @param {type: \"slider\", min:8, max:80}\n",
        "hist= model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), verbose=2,\n",
        "               callbacks=[model_checkpoint_callback],\n",
        "                          # , model_early_stopping_callback],\n",
        "                batch_size=64)\n",
        "# model.fit(train_generator, validation_data=val_generator, verbose=2, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6ocpNUdB4gus"
      },
      "outputs": [],
      "source": [
        "model.save('b1_poly_e=1_1_05.h5')\n",
        "model.save_weights('b1_poly_e=1_1_05_weights.h5')\n",
        "np.save('b1_poly_e=1_1_05.npy',hist.history)\n",
        "#history=np.load('my_history.npy',allow_pickle='TRUE').item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCmARmrwOBen"
      },
      "outputs": [],
      "source": [
        "model.save_weights('b1_CE_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "HyaifSSltxlH"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model = build_model(num_classes=100)\n",
        "#model.load_weights('b4_poly.h5')\n",
        "\n",
        "epochs = 43  # @param {type: \"slider\", min:8, max:80}\n",
        "hist= model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), verbose=2, batch_size=64)\n",
        "# model.fit(train_generator, validation_data=val_generator, verbose=2, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNbxfRbGJRWc"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs2gSo_-Uc3E"
      },
      "outputs": [],
      "source": [
        "def plot_hist(hist):\n",
        "    plt.plot(hist.history[\"accuracy\"])\n",
        "    plt.plot(hist.history[\"val_accuracy\"])\n",
        "    plt.title(\"model accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_hist(hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUq0fvaZ4inA"
      },
      "outputs": [],
      "source": [
        "history=np.load('b1_poly_smooth_history.npy',allow_pickle='TRUE').item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTBNvPtu64uF"
      },
      "outputs": [],
      "source": [
        "def plot_hist(hist):\n",
        "    plt.plot(hist[\"accuracy\"])\n",
        "    plt.plot(hist[\"val_accuracy\"])\n",
        "    plt.title(\"model accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_hist(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10ExzMN-Kx1k"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVxgQwfHi-1W"
      },
      "source": [
        "# **TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVQcppPFuI7i",
        "outputId": "7814ca6b-f56a-4ca2-ba31-5538d702db1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1280), dtype=tf.float32, name=None), name='batch_normalization_5/batchnorm/add_1:0', description=\"created by layer 'batch_normalization_5'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 1280), dtype=tf.float32, name=None), name='top_dropout/Identity:0', description=\"created by layer 'top_dropout'\")\n",
            "(None, 100)\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    model = build_model(num_classes=100)\n",
        "model.load_weights('best_b1_poly_e=1_1_05.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTKT1NmgwbWO"
      },
      "outputs": [],
      "source": [
        "\n",
        "############## HAM TEST #####################\n",
        "\n",
        "\n",
        "\n",
        "pred= model.predict(X_test,batch_size=64)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR3jNoecwlLi",
        "outputId": "bc6f7790-994b-495f-cb4f-7b8dfb9f6a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10 26  0 56]\n",
            "0\n",
            "[56 97  0 55]\n",
            "0\n",
            "[52 84 55 56]\n",
            "0\n",
            "[29 55  0 14]\n",
            "0\n",
            "[14 10 84 55]\n",
            "0\n",
            "[14 55 10 52]\n",
            "0\n",
            "[11 14 10 55]\n",
            "0\n",
            "[ 8 54  0 55]\n",
            "0\n",
            "[ 0 22 10 56]\n",
            "0\n",
            "[ 1 27 86 55]\n",
            "0\n",
            "[ 1  9 98 41]\n",
            "1\n",
            "[ 1 40 52 85]\n",
            "1\n",
            "[39 98 78 21]\n",
            "1\n",
            "[84  1 72 97]\n",
            "1\n",
            "[ 1 40 84  0]\n",
            "1\n",
            "[98  1 99 84]\n",
            "1\n",
            "[ 0  1 86 13]\n",
            "1\n",
            "[21 85  1 84]\n",
            "1\n",
            "[40  0 14 56]\n",
            "1\n",
            "[ 1 51 85 99]\n",
            "1\n",
            "[10 55 56 84]\n",
            "1\n",
            "[35 22 84  2]\n",
            "1\n",
            "[82 19  1 43]\n",
            "1\n",
            "[52  2 22 56]\n",
            "2\n",
            "[ 0 54 74 21]\n",
            "2\n",
            "[61 57 22  5]\n",
            "2\n",
            "[17  3  0 11]\n",
            "2\n",
            "[ 3  7  2 56]\n",
            "2\n",
            "[ 2 19 78  0]\n",
            "2\n",
            "[95 58 56 53]\n",
            "2\n",
            "[14 21  2 60]\n",
            "2\n",
            "[4 3 2 5]\n",
            "2\n",
            "[ 5 16  2 23]\n",
            "2\n",
            "[16  5  3 21]\n",
            "3\n",
            "[9 8 3 7]\n",
            "3\n",
            "[ 9 27  3 21]\n",
            "3\n",
            "[6 3 4 5]\n",
            "3\n",
            "[52  4  5 21]\n",
            "3\n",
            "[60 56  3  5]\n",
            "3\n",
            "[94 42  4  5]\n",
            "3\n",
            "[ 6  1 19  5]\n",
            "3\n",
            "[ 3 35 45  5]\n",
            "3\n",
            "[65  5 21  4]\n",
            "3\n",
            "[35 55  5  4]\n",
            "3\n",
            "[21  4  3  5]\n",
            "3\n",
            "[23 64 14 56]\n",
            "3\n",
            "[21  3  4  5]\n",
            "3\n",
            "[ 6 21  5  4]\n",
            "3\n",
            "[8 3 5 7]\n",
            "3\n",
            "[64  3 55  7]\n",
            "3\n",
            "[6 4 5 7]\n",
            "3\n",
            "[67  7  3  4]\n",
            "3\n",
            "[ 4  3 21  5]\n",
            "3\n",
            "[66  8  9  7]\n",
            "3\n",
            "[32 98 60  6]\n",
            "3\n",
            "[8 3 7 9]\n",
            "4\n",
            "[ 4 21  5  3]\n",
            "4\n",
            "[ 5 21  4  3]\n",
            "4\n",
            "[39  5  4  3]\n",
            "4\n",
            "[8 7 9 3]\n",
            "4\n",
            "[ 3  4 22 21]\n",
            "4\n",
            "[14 22  9 21]\n",
            "4\n",
            "[8 3 4 9]\n",
            "4\n",
            "[ 4 55  3  7]\n",
            "4\n",
            "[ 7 56 27 52]\n",
            "4\n",
            "[21  7  4 55]\n",
            "4\n",
            "[21  4  3  5]\n",
            "4\n",
            "[21  4  3  5]\n",
            "4\n",
            "[84  4  3 55]\n",
            "4\n",
            "[26 88 74 55]\n",
            "4\n",
            "[ 4 21  3  5]\n",
            "4\n",
            "[45  9  5  3]\n",
            "4\n",
            "[ 8 21  3  7]\n",
            "4\n",
            "[ 7 21  5  3]\n",
            "4\n",
            "[74 65 21  2]\n",
            "5\n",
            "[37  3  5  7]\n",
            "5\n",
            "[67  7  3 21]\n",
            "5\n",
            "[ 4 55  5  3]\n",
            "5\n",
            "[ 8  9 21  7]\n",
            "5\n",
            "[7 4 5 3]\n",
            "5\n",
            "[85 43 89  6]\n",
            "5\n",
            "[ 5  3 21  6]\n",
            "5\n",
            "[ 5 65  3  6]\n",
            "5\n",
            "[24  5  7  6]\n",
            "5\n",
            "[ 3 23 56  7]\n",
            "5\n",
            "[20 66  5  7]\n",
            "5\n",
            "[37  5 21  3]\n",
            "5\n",
            "[56  6  5  7]\n",
            "5\n",
            "[53 22  6 19]\n",
            "5\n",
            "[23 37  6  7]\n",
            "6\n",
            "[23 21  6  7]\n",
            "6\n",
            "[30 78  6 44]\n",
            "6\n",
            "[21  6  5  7]\n",
            "6\n",
            "[22 26  6 27]\n",
            "6\n",
            "[65 37  6  7]\n",
            "6\n",
            "[3 2 5 7]\n",
            "6\n",
            "[23  5  6  7]\n",
            "6\n",
            "[ 3 24  6  7]\n",
            "6\n",
            "[ 5 25  2  7]\n",
            "6\n",
            "[33 99  6  7]\n",
            "6\n",
            "[48  9  6  7]\n",
            "6\n",
            "[42  9 27  7]\n",
            "6\n",
            "[ 3  8 82 13]\n",
            "6\n",
            "[67 16 66 60]\n",
            "6\n",
            "[97 25 56  7]\n",
            "6\n",
            "[66  6  7  5]\n",
            "7\n",
            "[66 37  7  5]\n",
            "7\n",
            "[37  3  7  5]\n",
            "7\n",
            "[9 5 3 8]\n",
            "7\n",
            "[87  2 27 73]\n",
            "7\n",
            "[24  7  3  8]\n",
            "7\n",
            "[ 5 24 67  3]\n",
            "7\n",
            "[ 7 97  5  6]\n",
            "7\n",
            "[21  7  3  5]\n",
            "7\n",
            "[90  8 66  9]\n",
            "7\n",
            "[15 78 86 45]\n",
            "7\n",
            "[30 93  6 19]\n",
            "7\n",
            "[61  7  8  9]\n",
            "8\n",
            "[ 5  7 56  3]\n",
            "8\n",
            "[ 5 56 55  4]\n",
            "8\n",
            "[28  7  8  9]\n",
            "8\n",
            "[38 10  3  4]\n",
            "8\n",
            "[27 14  4  3]\n",
            "8\n",
            "[ 8 26  4  9]\n",
            "8\n",
            "[ 7 26  8  9]\n",
            "8\n",
            "[ 3 16  9  7]\n",
            "8\n",
            "[38  8  7 78]\n",
            "8\n",
            "[65 66 25  7]\n",
            "8\n",
            "[27  9  8  7]\n",
            "8\n",
            "[66  7  8  9]\n",
            "8\n",
            "[26  9 14  3]\n",
            "8\n",
            "[4 8 9 7]\n",
            "8\n",
            "[17  6  3  4]\n",
            "8\n",
            "[3 4 8 9]\n",
            "8\n",
            "[42 26  8 38]\n",
            "8\n",
            "[27 52 23  6]\n",
            "8\n",
            "[ 9  4  5 21]\n",
            "8\n",
            "[26 20 50 55]\n",
            "8\n",
            "[25  8  7  3]\n",
            "8\n",
            "[15  8  9  4]\n",
            "9\n",
            "[94  5  8 88]\n",
            "9\n",
            "[ 8  9 66  7]\n",
            "9\n",
            "[30 21 91 63]\n",
            "10\n",
            "[10 28 55 11]\n",
            "10\n",
            "[ 7 13 10 11]\n",
            "10\n",
            "[10 30 12 11]\n",
            "10\n",
            "[84 96  0 56]\n",
            "10\n",
            "[97 69  0 53]\n",
            "10\n",
            "[54  9 10 11]\n",
            "10\n",
            "[84 21 54 77]\n",
            "10\n",
            "[12 42  0 55]\n",
            "10\n",
            "[11 21 10 13]\n",
            "10\n",
            "[84  0 10 55]\n",
            "10\n",
            "[28  1  0 11]\n",
            "10\n",
            "[97 74 53 43]\n",
            "10\n",
            "[12 20 31 13]\n",
            "11\n",
            "[11 13 78 10]\n",
            "11\n",
            "[13 38 11 10]\n",
            "11\n",
            "[84  2 10 55]\n",
            "11\n",
            "[13 22 54 10]\n",
            "11\n",
            "[11 84 10 55]\n",
            "11\n",
            "[85 13 10 30]\n",
            "11\n",
            "[38 14 11 10]\n",
            "11\n",
            "[97 10 11 55]\n",
            "11\n",
            "[11 85 75 52]\n",
            "11\n",
            "[56 55 11 10]\n",
            "11\n",
            "[76 63 12 13]\n",
            "11\n",
            "[13 86  1 95]\n",
            "11\n",
            "[12 13 11 10]\n",
            "11\n",
            "[88  0 11 10]\n",
            "11\n",
            "[11 42 85 12]\n",
            "11\n",
            "[ 6 10 85 13]\n",
            "11\n",
            "[10 16 11 13]\n",
            "11\n",
            "[29 56 11 55]\n",
            "11\n",
            "[40 80  3 67]\n",
            "11\n",
            "[10 52 11 13]\n",
            "11\n",
            "[ 0 55 11 10]\n",
            "12\n",
            "[99 81 21 10]\n",
            "12\n",
            "[12 10 13 11]\n",
            "12\n",
            "[14 21 10 13]\n",
            "12\n",
            "[68  0 12 10]\n",
            "12\n",
            "[13 60 11 10]\n",
            "12\n",
            "[27 19 12 43]\n",
            "12\n",
            "[12 29 10 11]\n",
            "12\n",
            "[67 84 21 10]\n",
            "12\n",
            "[78 12 11 10]\n",
            "12\n",
            "[13 11 12 10]\n",
            "12\n",
            "[14 13 10 11]\n",
            "12\n",
            "[ 3 78 11 13]\n",
            "12\n",
            "[12 10  7 13]\n",
            "12\n",
            "[63 11 67 10]\n",
            "12\n",
            "[88 57 19 42]\n",
            "12\n",
            "[10 11 12 13]\n",
            "12\n",
            "[11 12  1 13]\n",
            "12\n",
            "[21 38 13 11]\n",
            "12\n",
            "[52  1 12 11]\n",
            "12\n",
            "[11 77 12 13]\n",
            "12\n",
            "[14 26 13 10]\n",
            "12\n",
            "[10 11 38 13]\n",
            "12\n",
            "[12 30 85 88]\n",
            "13\n",
            "[30 13 10 11]\n",
            "13\n",
            "[12 61 40 11]\n",
            "13\n",
            "[37 40 38 30]\n",
            "13\n",
            "[22  0 11 10]\n",
            "13\n",
            "[13 55 45 12]\n",
            "13\n",
            "[37 16 13 21]\n",
            "13\n",
            "[38 13 30 11]\n",
            "13\n",
            "[42  0 21 30]\n",
            "13\n",
            "[12  2 10 84]\n",
            "13\n",
            "[38 11 13 12]\n",
            "13\n",
            "[12 85 87 30]\n",
            "13\n",
            "[73 14 11 20]\n",
            "14\n",
            "[17 35 14 15]\n",
            "14\n",
            "[86 88 14 15]\n",
            "14\n",
            "[ 7 54  8 15]\n",
            "14\n",
            "[ 8 31 35 50]\n",
            "14\n",
            "[60 45 15  6]\n",
            "14\n",
            "[55 26 14 15]\n",
            "14\n",
            "[16 20 14 15]\n",
            "14\n",
            "[16  6 19 21]\n",
            "14\n",
            "[16 14  8 26]\n",
            "14\n",
            "[ 9 26 14 15]\n",
            "14\n",
            "[86 52 15 14]\n",
            "15\n",
            "[ 9 55 15 14]\n",
            "15\n",
            "[21 17 88 56]\n",
            "15\n",
            "[18  8 94 88]\n",
            "15\n",
            "[11 52 17  3]\n",
            "16\n",
            "[ 7 52 22 14]\n",
            "16\n",
            "[28 20 17 14]\n",
            "16\n",
            "[22 88 64 21]\n",
            "16\n",
            "[91 17 84 14]\n",
            "16\n",
            "[16 21 17 14]\n",
            "16\n",
            "[ 2 64 17 85]\n",
            "16\n",
            "[ 3  4 16  2]\n",
            "16\n",
            "[79 19 84 56]\n",
            "16\n",
            "[67 88 22  0]\n",
            "16\n",
            "[26  7 19  3]\n",
            "16\n",
            "[11 19 16 20]\n",
            "16\n",
            "[ 6 22 16 14]\n",
            "16\n",
            "[17 16 21 85]\n",
            "16\n",
            "[55 58 14 31]\n",
            "16\n",
            "[ 5  4 16 17]\n",
            "16\n",
            "[14 64 22  0]\n",
            "17\n",
            "[55 29 17 18]\n",
            "17\n",
            "[17 18 20 15]\n",
            "17\n",
            "[19 17 16 14]\n",
            "17\n",
            "[15 17 19 21]\n",
            "17\n",
            "[15 21 20 14]\n",
            "17\n",
            "[16 55 26 14]\n",
            "17\n",
            "[13 18 17 16]\n",
            "17\n",
            "[13 17 28 18]\n",
            "17\n",
            "[21 38 17 18]\n",
            "17\n",
            "[41 21 16 67]\n",
            "17\n",
            "[14 20 17 27]\n",
            "17\n",
            "[27 16 55 69]\n",
            "17\n",
            "[18 28 17 40]\n",
            "17\n",
            "[11 28 17 18]\n",
            "17\n",
            "[89 55 21 84]\n",
            "17\n",
            "[29 52 17 14]\n",
            "17\n",
            "[20 38 86 15]\n",
            "17\n",
            "[ 9 16 20 38]\n",
            "17\n",
            "[16 17 20 19]\n",
            "17\n",
            "[68 40 14 85]\n",
            "17\n",
            "[14 18 17 16]\n",
            "17\n",
            "[15 18 20  9]\n",
            "18\n",
            "[87 16 99 21]\n",
            "18\n",
            "[23 64 16 19]\n",
            "18\n",
            "[91 14  0  8]\n",
            "18\n",
            "[43 32 65 21]\n",
            "18\n",
            "[53  8 20 17]\n",
            "18\n",
            "[19 14 16 17]\n",
            "19\n",
            "[33 58 55  0]\n",
            "19\n",
            "[75 87 19 20]\n",
            "19\n",
            "[74  8 19 16]\n",
            "19\n",
            "[86 27 19 21]\n",
            "19\n",
            "[65 19 78 26]\n",
            "19\n",
            "[19 27 28 20]\n",
            "19\n",
            "[14 20 22  7]\n",
            "19\n",
            "[33 19 31 20]\n",
            "19\n",
            "[88 19 20 27]\n",
            "19\n",
            "[14 19 21 27]\n",
            "19\n",
            "[20 25 19 14]\n",
            "19\n",
            "[21 27 28 20]\n",
            "19\n",
            "[19 16 88 20]\n",
            "19\n",
            "[17 20 19 16]\n",
            "19\n",
            "[66 20 14 16]\n",
            "19\n",
            "[66 38 42 10]\n",
            "19\n",
            "[35 17 90 42]\n",
            "19\n",
            "[19 16 21 52]\n",
            "19\n",
            "[21  0 20 55]\n",
            "19\n",
            "[41 69 64 55]\n",
            "19\n",
            "[16 67 19 14]\n",
            "19\n",
            "[66 15 86 88]\n",
            "20\n",
            "[20 14 16 28]\n",
            "20\n",
            "[38 55 20 19]\n",
            "20\n",
            "[64 20 15 14]\n",
            "20\n",
            "[52 95 20 85]\n",
            "20\n",
            "[28 22 21 52]\n",
            "21\n",
            "[27 20 18 28]\n",
            "21\n",
            "[97 37 21 22]\n",
            "21\n",
            "[87 22 21 29]\n",
            "21\n",
            "[20 22 27 28]\n",
            "21\n",
            "[30 21 27 28]\n",
            "21\n",
            "[28 15 21 14]\n",
            "21\n",
            "[19 22 21 52]\n",
            "21\n",
            "[60 51 33 10]\n",
            "21\n",
            "[41 15 55 20]\n",
            "21\n",
            "[13 23 21 52]\n",
            "22\n",
            "[ 3 14 22 21]\n",
            "22\n",
            "[22  5 67 21]\n",
            "22\n",
            "[ 7 28 14 20]\n",
            "22\n",
            "[17 14 22 21]\n",
            "22\n",
            "[67 23 21 16]\n",
            "22\n",
            "[85  5 22 21]\n",
            "22\n",
            "[ 6 25 78 66]\n",
            "22\n",
            "[22 16 25 21]\n",
            "22\n",
            "[84 22 52 21]\n",
            "22\n",
            "[27 28 22 21]\n",
            "22\n",
            "[22 27 16 21]\n",
            "22\n",
            "[22 33 27 21]\n",
            "22\n",
            "[21 52 84 56]\n",
            "22\n",
            "[ 7 22 27 21]\n",
            "22\n",
            "[ 8 26 22 25]\n",
            "22\n",
            "[41 54 22 21]\n",
            "22\n",
            "[73 27 22 21]\n",
            "22\n",
            "[28 27 20 21]\n",
            "22\n",
            "[56 22 52 20]\n",
            "22\n",
            "[14 38 22 83]\n",
            "22\n",
            "[64 25 23 24]\n",
            "23\n",
            "[26 30 14 24]\n",
            "23\n",
            "[ 0 23 14 15]\n",
            "23\n",
            "[50 23 27 24]\n",
            "23\n",
            "[ 7 24 23  0]\n",
            "23\n",
            "[25  7 23 24]\n",
            "23\n",
            "[ 7 23 24 25]\n",
            "24\n",
            "[30  2 79 88]\n",
            "24\n",
            "[23  5 65 25]\n",
            "24\n",
            "[27 25 24 23]\n",
            "24\n",
            "[24 22 14 23]\n",
            "24\n",
            "[ 2  4 24 22]\n",
            "24\n",
            "[15  6 85 19]\n",
            "24\n",
            "[27 15  6 19]\n",
            "24\n",
            "[47 88 66 14]\n",
            "24\n",
            "[24  5 64 23]\n",
            "24\n",
            "[25 23 78  5]\n",
            "24\n",
            "[ 7 23 24 25]\n",
            "24\n",
            "[26 24 25 23]\n",
            "24\n",
            "[ 7 78 24 23]\n",
            "24\n",
            "[25 22 23 64]\n",
            "24\n",
            "[14  7 25 23]\n",
            "25\n",
            "[22 15  8 26]\n",
            "25\n",
            "[69 79 19 17]\n",
            "25\n",
            "[52 25 23 24]\n",
            "25\n",
            "[42 23 25 24]\n",
            "25\n",
            "[ 7 55 14 24]\n",
            "25\n",
            "[14 21 25  7]\n",
            "25\n",
            "[43 53 46 26]\n",
            "25\n",
            "[38 41 56  8]\n",
            "25\n",
            "[20 21 25 14]\n",
            "25\n",
            "[ 8 24 31 26]\n",
            "25\n",
            "[43 74 26 46]\n",
            "25\n",
            "[15  5 25 24]\n",
            "25\n",
            "[ 8 20  9 26]\n",
            "25\n",
            "[ 6 46 23 17]\n",
            "26\n",
            "[27  8 14 25]\n",
            "26\n",
            "[26 46  4 25]\n",
            "26\n",
            "[ 6 19 76 17]\n",
            "26\n",
            "[24 26 25 32]\n",
            "26\n",
            "[56 76 21 16]\n",
            "26\n",
            "[41  9 26 25]\n",
            "26\n",
            "[92 71 35 60]\n",
            "26\n",
            "[74 16 43 24]\n",
            "26\n",
            "[89 57 25 28]\n",
            "27\n",
            "[14 20 27 28]\n",
            "27\n",
            "[ 7 99 24 43]\n",
            "27\n",
            "[21 27 28 52]\n",
            "27\n",
            "[79 73 22 20]\n",
            "27\n",
            "[63 25 66 28]\n",
            "27\n",
            "[32 30 27 28]\n",
            "27\n",
            "[27 28 66 30]\n",
            "27\n",
            "[20 27 22 21]\n",
            "27\n",
            "[30 14 27 28]\n",
            "27\n",
            "[30 28 27 21]\n",
            "27\n",
            "[57 38 77 46]\n",
            "27\n",
            "[30 15 27 28]\n",
            "27\n",
            "[31 28 25 27]\n",
            "28\n",
            "[27 55 28 21]\n",
            "28\n",
            "[18 41 74 54]\n",
            "28\n",
            "[16 28 21 27]\n",
            "28\n",
            "[20 90 73 65]\n",
            "28\n",
            "[23 21 28 27]\n",
            "28\n",
            "[30 21 28 27]\n",
            "28\n",
            "[66 21 28 27]\n",
            "28\n",
            "[19 21 43 29]\n",
            "28\n",
            "[50 46 30 53]\n",
            "28\n",
            "[86 14 28 15]\n",
            "28\n",
            "[ 9 30 28 27]\n",
            "28\n",
            "[21 31 26 30]\n",
            "28\n",
            "[ 1 46 21 65]\n",
            "28\n",
            "[30 28 27 32]\n",
            "28\n",
            "[20 28 21 27]\n",
            "28\n",
            "[81 50 26 17]\n",
            "28\n",
            "[30 20 28 27]\n",
            "28\n",
            "[26 20 28 27]\n",
            "28\n",
            "[63 27 30 21]\n",
            "28\n",
            "[26 30 29 28]\n",
            "29\n",
            "[35 28 30 14]\n",
            "29\n",
            "[80 78 29 30]\n",
            "29\n",
            "[21 30 29 14]\n",
            "29\n",
            "[24 31 29 30]\n",
            "29\n",
            "[67 31 29 30]\n",
            "29\n",
            "[27  1 29 30]\n",
            "29\n",
            "[37 31 29 30]\n",
            "29\n",
            "[20 14 30 21]\n",
            "29\n",
            "[27 32 28 30]\n",
            "29\n",
            "[21 37 31 30]\n",
            "29\n",
            "[50 33 31 30]\n",
            "29\n",
            "[33 29 31 30]\n",
            "29\n",
            "[83 78 13 30]\n",
            "29\n",
            "[84 30 85 41]\n",
            "29\n",
            "[21 29 30 27]\n",
            "29\n",
            "[60 55 67  3]\n",
            "29\n",
            "[28 32 30 15]\n",
            "30\n",
            "[24 66 30 27]\n",
            "30\n",
            "[ 7 99 60 27]\n",
            "30\n",
            "[56 30 29 31]\n",
            "30\n",
            "[94 33 30 29]\n",
            "30\n",
            "[14 30 29 28]\n",
            "30\n",
            "[33 52 30 13]\n",
            "30\n",
            "[21 28 30 29]\n",
            "30\n",
            "[79 13 12 29]\n",
            "31\n",
            "[37 31 32 30]\n",
            "31\n",
            "[95 30 74 83]\n",
            "31\n",
            "[25 33 31 30]\n",
            "31\n",
            "[55  0  3  1]\n",
            "31\n",
            "[79 29 24 13]\n",
            "31\n",
            "[31 32 52 30]\n",
            "31\n",
            "[26 32 17 18]\n",
            "32\n",
            "[30 26 32 31]\n",
            "32\n",
            "[25 32 20 26]\n",
            "32\n",
            "[87 24 95 25]\n",
            "32\n",
            "[50 23  7 35]\n",
            "32\n",
            "[38 31 32 30]\n",
            "32\n",
            "[32 65 26 18]\n",
            "32\n",
            "[37 27 31 33]\n",
            "32\n",
            "[12 24 88 45]\n",
            "32\n",
            "[26 85 28 45]\n",
            "32\n",
            "[14 30 32 31]\n",
            "32\n",
            "[28 61 32 30]\n",
            "32\n",
            "[48  2 17 42]\n",
            "32\n",
            "[88 11 43 21]\n",
            "33\n",
            "[11 99 43 29]\n",
            "33\n",
            "[23 92 42 55]\n",
            "33\n",
            "[23 26 92 91]\n",
            "33\n",
            "[17 60 26 43]\n",
            "33\n",
            "[63  0 28 42]\n",
            "33\n",
            "[45 32  0 55]\n",
            "33\n",
            "[17 55 34 35]\n",
            "34\n",
            "[52 94 93 42]\n",
            "34\n",
            "[35 27 63 93]\n",
            "34\n",
            "[35 37 34 69]\n",
            "34\n",
            "[93 50 34 61]\n",
            "34\n",
            "[15 86 34 35]\n",
            "34\n",
            "[61 93 38 35]\n",
            "34\n",
            "[17 94 88 77]\n",
            "35\n",
            "[17 93 37 34]\n",
            "35\n",
            "[17 67 90 57]\n",
            "36\n",
            "[36 57 50 43]\n",
            "36\n",
            "[78 50 27 38]\n",
            "37\n",
            "[14 83 37 38]\n",
            "37\n",
            "[21 73 37 38]\n",
            "37\n",
            "[39 11 37 38]\n",
            "37\n",
            "[37 66 38 30]\n",
            "37\n",
            "[45 21 37 38]\n",
            "37\n",
            "[30 42 37 38]\n",
            "37\n",
            "[93  9 61 38]\n",
            "37\n",
            "[39 21 37 38]\n",
            "37\n",
            "[74 84 37 38]\n",
            "37\n",
            "[74 93 37 38]\n",
            "37\n",
            "[83 78 37 38]\n",
            "37\n",
            "[36 21 37 42]\n",
            "37\n",
            "[46 91 98 65]\n",
            "37\n",
            "[ 1 47 40 56]\n",
            "37\n",
            "[83 56 37 38]\n",
            "37\n",
            "[66 91 38 37]\n",
            "38\n",
            "[77 38  6 44]\n",
            "38\n",
            "[ 7 56 38 37]\n",
            "38\n",
            "[83 65 38 37]\n",
            "38\n",
            "[95 38 86 85]\n",
            "38\n",
            "[ 7 60 20 65]\n",
            "38\n",
            "[96 74 38 37]\n",
            "38\n",
            "[51 56 90 74]\n",
            "39\n",
            "[79 50 39 70]\n",
            "39\n",
            "[21  2 97 50]\n",
            "39\n",
            "[79 39 31 74]\n",
            "39\n",
            "[97  8 39 40]\n",
            "39\n",
            "[40 93 26 74]\n",
            "40\n",
            "[ 1 41 76 56]\n",
            "41\n",
            "[78 41 87 86]\n",
            "41\n",
            "[94 78 41 56]\n",
            "41\n",
            "[55  0 56 86]\n",
            "41\n",
            "[ 9  1 86 56]\n",
            "41\n",
            "[56 86 41  1]\n",
            "41\n",
            "[31 88 41 56]\n",
            "41\n",
            "[88 87 41 86]\n",
            "41\n",
            "[54 55 56 86]\n",
            "41\n",
            "[43 91 41 86]\n",
            "41\n",
            "[88 10 86 56]\n",
            "41\n",
            "[94 65 41 86]\n",
            "41\n",
            "[86 76 41 56]\n",
            "41\n",
            "[56 87 41 86]\n",
            "41\n",
            "[3 9 8 4]\n",
            "41\n",
            "[80 88 86 56]\n",
            "41\n",
            "[41 87 76 56]\n",
            "41\n",
            "[14 88 41 56]\n",
            "41\n",
            "[ 7 37 42 67]\n",
            "42\n",
            "[ 1 56 48 43]\n",
            "42\n",
            "[40 38 48  1]\n",
            "42\n",
            "[11 98 42  0]\n",
            "42\n",
            "[50 20 42 77]\n",
            "42\n",
            "[57 22 43 53]\n",
            "43\n",
            "[14 88 43 53]\n",
            "43\n",
            "[82 88 43 53]\n",
            "43\n",
            "[57 43 88 53]\n",
            "43\n",
            "[54 57 43 53]\n",
            "43\n",
            "[40 54 43 53]\n",
            "43\n",
            "[43 50  6 53]\n",
            "43\n",
            "[67 43 53 54]\n",
            "43\n",
            "[36 43 53 57]\n",
            "43\n",
            "[43 54 67 42]\n",
            "43\n",
            "[43 86 88 53]\n",
            "43\n",
            "[88 57 43 53]\n",
            "43\n",
            "[40 90 43 50]\n",
            "43\n",
            "[88 95 43 53]\n",
            "43\n",
            "[57 95 56 53]\n",
            "43\n",
            "[41 68 44 46]\n",
            "44\n",
            "[78 46 44 56]\n",
            "44\n",
            "[80 45 44 88]\n",
            "44\n",
            "[94 45 44 88]\n",
            "44\n",
            "[56 45 44 74]\n",
            "44\n",
            "[78 45 44 46]\n",
            "44\n",
            "[86 55 66 65]\n",
            "44\n",
            "[74 46 44 45]\n",
            "44\n",
            "[66 44 45 46]\n",
            "45\n",
            "[73 86 45 69]\n",
            "45\n",
            "[88 45 69 46]\n",
            "45\n",
            "[76 45 66 46]\n",
            "45\n",
            "[65 88 86 46]\n",
            "45\n",
            "[78 69 75 46]\n",
            "45\n",
            "[52 45 46 69]\n",
            "45\n",
            "[43 29 94 97]\n",
            "45\n",
            "[29 74 45 46]\n",
            "45\n",
            "[55 99 45 46]\n",
            "45\n",
            "[84 45 46 86]\n",
            "45\n",
            "[79 45 55 46]\n",
            "45\n",
            "[70 69 46 41]\n",
            "46\n",
            "[55 19 46 45]\n",
            "46\n",
            "[88 65 46 45]\n",
            "46\n",
            "[69 46  2 38]\n",
            "46\n",
            "[86 46 45 88]\n",
            "46\n",
            "[46 74 88 45]\n",
            "46\n",
            "[47 32 10 37]\n",
            "47\n",
            "[43 56 47 57]\n",
            "47\n",
            "[40 89 47 48]\n",
            "47\n",
            "[74 95 48 47]\n",
            "48\n",
            "[ 7 87  0 57]\n",
            "48\n",
            "[91 81 63 90]\n",
            "48\n",
            "[66 54 70 80]\n",
            "49\n",
            "[32 51 49 50]\n",
            "49\n",
            "[49 93 61 90]\n",
            "49\n",
            "[36 49 39 50]\n",
            "49\n",
            "[76 45 69 70]\n",
            "49\n",
            "[20 80 49 50]\n",
            "49\n",
            "[92 90 49 50]\n",
            "49\n",
            "[67 41 24 98]\n",
            "50\n",
            "[ 9 28 17 92]\n",
            "50\n",
            "[49 52 80 56]\n",
            "50\n",
            "[92 51 39 50]\n",
            "51\n",
            "[52 70 49 74]\n",
            "51\n",
            "[79 17 19 46]\n",
            "51\n",
            "[79 78 80 81]\n",
            "51\n",
            "[52 85 43 95]\n",
            "52\n",
            "[74 21 84 47]\n",
            "52\n",
            "[22 52 14 85]\n",
            "52\n",
            "[96 84 52 85]\n",
            "52\n",
            "[10 84 14 85]\n",
            "52\n",
            "[93  2 73 55]\n",
            "52\n",
            "[33 84 52 55]\n",
            "52\n",
            "[24 85 52 84]\n",
            "52\n",
            "[85 84 21  0]\n",
            "52\n",
            "[85 74 26 43]\n",
            "53\n",
            "[53  0 22 43]\n",
            "53\n",
            "[74 30 53 43]\n",
            "53\n",
            "[90 53 55 46]\n",
            "53\n",
            "[85 54 53 43]\n",
            "53\n",
            "[82 57 53 43]\n",
            "53\n",
            "[59 95  6 43]\n",
            "53\n",
            "[ 1 95 19 43]\n",
            "53\n",
            "[95 57 53 43]\n",
            "53\n",
            "[95 82 53 57]\n",
            "53\n",
            "[57 54 53 43]\n",
            "53\n",
            "[43 67 53 90]\n",
            "53\n",
            "[57 54 53 43]\n",
            "53\n",
            "[74 53 67 43]\n",
            "53\n",
            "[69 43 85  0]\n",
            "54\n",
            "[38 43 54 42]\n",
            "54\n",
            "[80 21 54  7]\n",
            "54\n",
            "[ 0 21 54 16]\n",
            "54\n",
            "[90 45 55  0]\n",
            "54\n",
            "[54 56 42 67]\n",
            "54\n",
            "[17 55 26 32]\n",
            "55\n",
            "[10 65 56  0]\n",
            "55\n",
            "[86 52 55 14]\n",
            "55\n",
            "[88 56 55 14]\n",
            "55\n",
            "[44 61 16 85]\n",
            "55\n",
            "[74 16 55 21]\n",
            "55\n",
            "[ 1  0 51 41]\n",
            "56\n",
            "[78 55  2  4]\n",
            "56\n",
            "[14 43 21 86]\n",
            "56\n",
            "[35  1 57 95]\n",
            "57\n",
            "[72 78  0 53]\n",
            "58\n",
            "[ 1 47 57 95]\n",
            "58\n",
            "[53 94 58 57]\n",
            "58\n",
            "[ 1 57 58 95]\n",
            "58\n",
            "[ 1 62 58 57]\n",
            "58\n",
            "[49 73 57 51]\n",
            "58\n",
            "[11 48 59 47]\n",
            "59\n",
            "[49 63 59 60]\n",
            "59\n",
            "[57 74 77 47]\n",
            "59\n",
            "[50 39 67 92]\n",
            "59\n",
            "[89 55 59 60]\n",
            "59\n",
            "[37 45 61 77]\n",
            "60\n",
            "[56 60 61 77]\n",
            "60\n",
            "[88 77 60 61]\n",
            "60\n",
            "[42 93 60 61]\n",
            "60\n",
            "[96 63 42 73]\n",
            "60\n",
            "[ 7 61 67 60]\n",
            "61\n",
            "[55 93 61 77]\n",
            "61\n",
            "[94 61 60 93]\n",
            "61\n",
            "[85 51 71 60]\n",
            "61\n",
            "[78 42 61 60]\n",
            "61\n",
            "[99 61 60 93]\n",
            "61\n",
            "[62 98 45 23]\n",
            "62\n",
            "[24 36 25 87]\n",
            "63\n",
            "[61 31 63 34]\n",
            "63\n",
            "[20 69 46 50]\n",
            "64\n",
            "[11  5 64 65]\n",
            "64\n",
            "[ 4  5 24 25]\n",
            "64\n",
            "[26 29 43 71]\n",
            "64\n",
            "[ 5 66 64 65]\n",
            "64\n",
            "[45 31 66 69]\n",
            "64\n",
            "[43 28 64 65]\n",
            "64\n",
            "[34 74 64 65]\n",
            "64\n",
            "[23 10 22 27]\n",
            "64\n",
            "[13 64 69 65]\n",
            "64\n",
            "[23 99 64 65]\n",
            "64\n",
            "[14 78 64 65]\n",
            "64\n",
            "[46 65 27 66]\n",
            "65\n",
            "[38  9 65 66]\n",
            "65\n",
            "[ 8  7 65 66]\n",
            "65\n",
            "[13 64 65 27]\n",
            "65\n",
            "[93 46 65 66]\n",
            "65\n",
            "[75 52 22 66]\n",
            "65\n",
            "[99 66 65  7]\n",
            "65\n",
            "[14 65 66  9]\n",
            "65\n",
            "[83 64  9  6]\n",
            "66\n",
            "[17 85 41  6]\n",
            "66\n",
            "[26 99 66 65]\n",
            "66\n",
            "[14 55 66 65]\n",
            "66\n",
            "[ 3 65 66 46]\n",
            "66\n",
            "[21 65 78 87]\n",
            "66\n",
            "[99 46 66 65]\n",
            "66\n",
            "[14 45 66 65]\n",
            "66\n",
            "[14 45 66 65]\n",
            "66\n",
            "[97 14 55 65]\n",
            "66\n",
            "[32 55 66 65]\n",
            "66\n",
            "[14 28 20 31]\n",
            "67\n",
            "[90 67 94 60]\n",
            "67\n",
            "[ 1 67 88 94]\n",
            "67\n",
            "[37 94 67  8]\n",
            "67\n",
            "[97 34 77 14]\n",
            "67\n",
            "[81 78 68 69]\n",
            "68\n",
            "[62 88 68 69]\n",
            "68\n",
            "[73  7 87 86]\n",
            "68\n",
            "[44 98 68 69]\n",
            "68\n",
            "[97 27 68 69]\n",
            "68\n",
            "[ 1 68 87 69]\n",
            "68\n",
            "[78 56 68 69]\n",
            "68\n",
            "[62 31 68 69]\n",
            "68\n",
            "[41 44 68 69]\n",
            "68\n",
            "[11 56 68 69]\n",
            "68\n",
            "[97 19 20 88]\n",
            "69\n",
            "[50 17 20 46]\n",
            "69\n",
            "[69 80 81 73]\n",
            "70\n",
            "[79 70 69 74]\n",
            "70\n",
            "[68 79 70 81]\n",
            "70\n",
            "[16 69 70 79]\n",
            "70\n",
            "[88 70 66 45]\n",
            "70\n",
            "[97 79 69 68]\n",
            "70\n",
            "[74 43 70 97]\n",
            "70\n",
            "[72 83 63 80]\n",
            "72\n",
            "[20 71 72 96]\n",
            "72\n",
            "[10 73 96 78]\n",
            "73\n",
            "[ 3 73 82 72]\n",
            "73\n",
            "[71 50 60 36]\n",
            "73\n",
            "[96 98 74 75]\n",
            "74\n",
            "[74 81 92 50]\n",
            "74\n",
            "[80 20 79 74]\n",
            "75\n",
            "[81 69 76 74]\n",
            "75\n",
            "[75 14 80 74]\n",
            "75\n",
            "[26 87 76 56]\n",
            "76\n",
            "[38 55 12 54]\n",
            "76\n",
            "[56  9 76 78]\n",
            "76\n",
            "[76 86 88 56]\n",
            "76\n",
            "[ 1 86 76 56]\n",
            "76\n",
            "[98 72 22 56]\n",
            "76\n",
            "[38 21 76 86]\n",
            "76\n",
            "[97 78 76 56]\n",
            "76\n",
            "[41 88 86 78]\n",
            "76\n",
            "[86 76 78 56]\n",
            "76\n",
            "[80 74 76 56]\n",
            "76\n",
            "[79 76 78 56]\n",
            "76\n",
            "[94 77 93 57]\n",
            "77\n",
            "[94 77 42 61]\n",
            "77\n",
            "[69  2 99  4]\n",
            "77\n",
            "[96  0 53 54]\n",
            "77\n",
            "[34 42 77 61]\n",
            "77\n",
            "[17  8 78 65]\n",
            "78\n",
            "[97 76 78 56]\n",
            "78\n",
            "[25 19 21 43]\n",
            "78\n",
            "[80 74 46 70]\n",
            "79\n",
            "[99 78 79 87]\n",
            "79\n",
            "[44 79 68 70]\n",
            "79\n",
            "[26 79 32 81]\n",
            "80\n",
            "[26 64 80 86]\n",
            "80\n",
            "[18 43 80 81]\n",
            "80\n",
            "[96 41 42 88]\n",
            "80\n",
            "[64 28 29  1]\n",
            "81\n",
            "[41  7 50 74]\n",
            "81\n",
            "[85 88 81 80]\n",
            "81\n",
            "[86 79 80 70]\n",
            "81\n",
            "[88 86 81 80]\n",
            "81\n",
            "[54 79 81 80]\n",
            "81\n",
            "[ 8 58 82 91]\n",
            "82\n",
            "[35 95 53 57]\n",
            "82\n",
            "[49 22 82 95]\n",
            "82\n",
            "[55 45 91 29]\n",
            "82\n",
            "[56 39  0 42]\n",
            "83\n",
            "[30 23 83 37]\n",
            "83\n",
            "[26 30 46 43]\n",
            "83\n",
            "[83 38 59 47]\n",
            "83\n",
            "[ 8 91 92 49]\n",
            "84\n",
            "[17 84 55 85]\n",
            "84\n",
            "[ 3 55 84 52]\n",
            "84\n",
            "[56  3  1 43]\n",
            "84\n",
            "[85 84 55 28]\n",
            "84\n",
            "[78 13 52 59]\n",
            "85\n",
            "[76 85 26 52]\n",
            "85\n",
            "[38 13  5 52]\n",
            "85\n",
            "[20 84 28 52]\n",
            "85\n",
            "[18 84 85 52]\n",
            "85\n",
            "[50 98 85 52]\n",
            "85\n",
            "[ 0 16 85 52]\n",
            "85\n",
            "[27 85 84 52]\n",
            "85\n",
            "[20 18 85 87]\n",
            "85\n",
            "[84 28 85 52]\n",
            "85\n",
            "[20 28 85 54]\n",
            "85\n",
            "[54 19 98 52]\n",
            "85\n",
            "[86 28 14 52]\n",
            "85\n",
            "[20 84 85 52]\n",
            "85\n",
            "[78 85 56 52]\n",
            "85\n",
            "[37 85 10 52]\n",
            "85\n",
            "[88  0 85 21]\n",
            "85\n",
            "[52  9 85 84]\n",
            "85\n",
            "[10 21 85 52]\n",
            "85\n",
            "[56 86 87 88]\n",
            "86\n",
            "[88  1 86 56]\n",
            "86\n",
            "[67 26 86 88]\n",
            "86\n",
            "[41 56 86 88]\n",
            "86\n",
            "[76 88 86 56]\n",
            "86\n",
            "[39 56 86 88]\n",
            "86\n",
            "[86 10 88 87]\n",
            "86\n",
            "[38 86 55 41]\n",
            "86\n",
            "[87 56 86 88]\n",
            "86\n",
            "[ 7 86 88 41]\n",
            "86\n",
            "[87 78 76 56]\n",
            "86\n",
            "[41 86 88 87]\n",
            "86\n",
            "[22  1 76 56]\n",
            "86\n",
            "[25 88 86 56]\n",
            "86\n",
            "[98 86 88 56]\n",
            "86\n",
            "[87 56 86 88]\n",
            "86\n",
            "[81 88 86 79]\n",
            "87\n",
            "[87 50 56 45]\n",
            "87\n",
            "[29 99 86 88]\n",
            "87\n",
            "[15 87 88 56]\n",
            "87\n",
            "[89 15 88 86]\n",
            "87\n",
            "[74 79 86 56]\n",
            "87\n",
            "[87 56 88 86]\n",
            "87\n",
            "[88 87 56 86]\n",
            "87\n",
            "[76 74 86 56]\n",
            "87\n",
            "[ 2 46 87 32]\n",
            "87\n",
            "[94 86 87 88]\n",
            "87\n",
            "[75 87 56 79]\n",
            "87\n",
            "[88 97 86 56]\n",
            "87\n",
            "[88 38 41 56]\n",
            "87\n",
            "[ 0 86 56 88]\n",
            "87\n",
            "[88 52 86 56]\n",
            "87\n",
            "[56 88 74 86]\n",
            "87\n",
            "[55 88 52 86]\n",
            "88\n",
            "[63 53 88 86]\n",
            "88\n",
            "[36 74 88 86]\n",
            "88\n",
            "[56 53 88 86]\n",
            "88\n",
            "[46 69 88 86]\n",
            "88\n",
            "[28 87 88 86]\n",
            "88\n",
            "[84 56 88 86]\n",
            "88\n",
            "[41 45 69 86]\n",
            "88\n",
            "[18 88 86 87]\n",
            "88\n",
            "[14 87 60 81]\n",
            "88\n",
            "[41 87 15 78]\n",
            "88\n",
            "[84 86  1  6]\n",
            "88\n",
            "[18 87 88 86]\n",
            "88\n",
            "[60 40 89 67]\n",
            "89\n",
            "[21 67 89 27]\n",
            "89\n",
            "[80 40 89 67]\n",
            "89\n",
            "[60 89 66 90]\n",
            "89\n",
            "[34 58 28 93]\n",
            "89\n",
            "[39 57 89 47]\n",
            "90\n",
            "[11 81 12 97]\n",
            "90\n",
            "[21 39 90 67]\n",
            "90\n",
            "[91 70 90 54]\n",
            "90\n",
            "[39 91 90 47]\n",
            "90\n",
            "[73 84 54 21]\n",
            "90\n",
            "[57 95 73 61]\n",
            "91\n",
            "[55 35  0 59]\n",
            "91\n",
            "[67 55  9  0]\n",
            "94\n",
            "[94 21 67 40]\n",
            "94\n",
            "[62  0 94 93]\n",
            "94\n",
            "[88 26 94 93]\n",
            "94\n",
            "[96 92 53 57]\n",
            "95\n",
            "[57 96 71 49]\n",
            "95\n",
            "[95 32 49 79]\n",
            "95\n",
            "[33  0 22 48]\n",
            "95\n",
            "[89  1 43 56]\n",
            "95\n",
            "[55 56 96 42]\n",
            "96\n",
            "[55 10 17  0]\n",
            "97\n",
            "[81 78 97 98]\n",
            "97\n",
            "[78 25 60 42]\n",
            "98\n",
            "[ 1 86 98 56]\n",
            "98\n",
            "[79 98 56 97]\n",
            "98\n",
            "[98 86 99 84]\n",
            "98\n",
            "[22 91 98  1]\n",
            "98\n",
            "[88 56 98 84]\n",
            "98\n",
            "[74  6 46 90]\n",
            "98\n",
            "[54 98 99 84]\n",
            "99\n",
            "[78 38 19 69]\n",
            "99\n",
            "[93 55 81 46]\n",
            "99\n",
            "0.7431743174317432\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1551155115511551"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "score=0\n",
        "addition=0\n",
        "for i in range(pred.shape[0]):\n",
        "  if np.argmax(pred[i])!=np.argmax(y_test[i]):\n",
        "    sorted_index_array = np.argsort(pred[i])\n",
        "  \n",
        "    # sorted array\n",
        "    n =4\n",
        "    sorted_array = pred[i][sorted_index_array[-n : ]]\n",
        "    # pred[0]\n",
        "    \n",
        "      \n",
        "    # we are using negative\n",
        "    # indexing concept\n",
        "      \n",
        "    # find n largest value\n",
        "    rslt = sorted_array[-n : ]\n",
        "    print(sorted_index_array[-n:])\n",
        "    print(np.argmax(y_test[i]))\n",
        "    if np.argmax(y_test[i]) in sorted_index_array[-n:]:\n",
        "      addition+=1\n",
        "\n",
        "    #print(rslt)\n",
        "  else:\n",
        "    score+=1\n",
        "score/=3333\n",
        "print(score)\n",
        "addition/3333\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RhXEA0KxU0T",
        "outputId": "09a12c2f-1680-4815-df83-578baa1893d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.77"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "p= np.zeros(pred.shape[0])\n",
        "t= np.zeros(y_test.shape[0])\n",
        "for i in range(pred.shape[0]):\n",
        "    p[i]=np.argmax(pred[i])\n",
        "    t[i]= np.argmax(y_test[i])\n",
        "\n",
        "m=confusion_matrix(p,t)\n",
        "m.trace()/m.shape[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emdhV8ug4IvY"
      },
      "outputs": [],
      "source": [
        "pred[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bgXhqU4-8d-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.activations import softmax\n",
        "x=tf.constant([[-10, -5, 0.0, 5, 10]], dtype = tf.float32)\n",
        "#x=tf.keras.layers.Flatten()(x)\n",
        "#print(x)\n",
        "x=layers.Dense(10, activation='relu', name='dense_1')(x)\n",
        "#print(x)\n",
        "#print(x.numpy())\n",
        "x = layers.Reshape((2,5))(x)\n",
        "# arr= np.array([])\n",
        "# for i in range(x.shape[0]):\n",
        "#     tmp=layers.Dense(x.shape[2], activation='softmax', name='dense_2')( tf.convert_to_tensor(x[i]))\n",
        "#     arr=np.append(arr,tmp.numpy())\n",
        "# # arr= arr[:-2]\n",
        "# print(np.array([arr]))\n",
        "# y=np.array([arr])\n",
        "# # x[0,1]\n",
        "# x=tf.convert_to_tensor(y)\n",
        "x = softmax(x,axis=2)\n",
        "x\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0_Kh_s9CSNJ"
      },
      "outputs": [],
      "source": [
        "x[0,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTyLbnJbDpRY"
      },
      "outputs": [],
      "source": [
        "ex1 = tf.convert_to_tensor([[2,-1,1],[-1,2,0],[1,0,2]],dtype=tf.float32)\n",
        "\n",
        "tf.linalg.eigvals(ex1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Copy of Copy of EfficientNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}